{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymxatB5WYxlL"
   },
   "source": [
    "# Transformer ì‹¤ìŠµ\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ê°ì • ë¶„ì„ taskì— RNN ëŒ€ì‹  Transformerë¥¼ êµ¬í˜„í•˜ì—¬ ì ìš©í•´ ë³¼ ê²ƒì…ë‹ˆë‹¤.\n",
    "Library importë‚˜ dataloader ìƒì„±ì€ RNN ì‹¤ìŠµ ë•Œì™€ ë˜‘ê°™ê¸° ë•Œë¬¸ì— ì„¤ëª…ì€ ë„˜ì–´ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOdhoBVA1zcu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(row['label'])\n",
    "    texts.append(row['text'])\n",
    "\n",
    "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-FshZcTZBQ2"
   },
   "source": [
    "## Self-Attention (ì£¼ì„ ì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBlMVMZcRAxv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "# ì…€í”„ ì–´í…ì…˜\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim # ì…ë ¥ ì°¨ì›  (ë³´í†µ ì–´íœ˜ í¬ê¸°((vocab_size)))\n",
    "    self.d_model = d_model     # ì„ë² ë”© ì°¨ì› (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥´ë‹¤. BERTëŠ” 768..)\n",
    "\n",
    "    self.wq = nn.Linear(input_dim, d_model)  # ì¿¼ë¦¬(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wk = nn.Linear(input_dim, d_model)  #   í‚¤(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wv = nn.Linear(input_dim, d_model)  #   ê°’(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "\n",
    "    # ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ (d_model -> d_model)\n",
    "    self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # ì†Œí”„íŠ¸ë§¥ìŠ¤ (ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•´((ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ìˆ˜í–‰)))\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    ### 1. Q, K, V ë²¡í„° ìƒì„± â€“ ì…ë ¥ì„ í†µí•´ Wq, Wk, Wvë¥¼ ì„ í˜• ë³€í™˜í•˜ì—¬ Q, K, Vë¥¼ ë§Œë“¬\n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\t# Q (Query): í˜„ì¬ ê¸°ì¤€ì´ ë˜ëŠ” í† í° (ê²€ìƒ‰ ê¸°ì¤€)\n",
    "    # K (Key): ëª¨ë“  í† í° (ê¸°ì¤€ê³¼ ë¹„êµí•  ëŒ€ìƒ)\n",
    "    # V (Value): ìµœì¢…ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ ë°˜í™˜í•  ê°’\n",
    "    # ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì€ Qì™€ Kì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ Vì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì–¼ë§ˆë§Œí¼ ê°€ì ¸ì˜¬ì§€ ê²°ì •í•œë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "    ### 2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° â€“ ì¿¼ë¦¬(Q)ì™€ í‚¤(K)ë¥¼ ë‚´ì í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° (Q * K^T)\n",
    "    # W_qì™€ W_këŠ” ë¬¸ë§¥ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ë†’ì€ ë‚´ì  ê°’ì„ ë‚´ë±‰ë„ë¡ í•™ìŠµë‡Œë‹¤.\n",
    "    # B(Batch Size: ë°°ì¹˜ í¬ê¸°), S(Sequence length: ì‹œí€€ìŠ¤ ê¸¸ì´), D(Embedding Dimension: ì„ë² ë”© ì°¨ì›)\n",
    "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "    # ì–´í…ì…˜ì—ì„œ ë‚´ì  ê²°ê³¼ë¥¼ ì—ë„ˆì§€ë¼ê³ ë„ í•œë‹¤.\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ í¬ë©´: ì¿¼ë¦¬ì™€ í‚¤ê°€ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ ì‘ì„ìˆ˜ë¡ ë‘ ë²¡í„°ê°„ì˜ ìƒê´€ì„±ì´ ë‚®ë‹¤ëŠ” ì˜ë¯¸\n",
    "\n",
    "    ### 3. ìŠ¤ì½”ì–´ ì •ê·œí™” â€“ sqrt(d_model)ë¡œ ë‚˜ëˆ ì„œ ìŠ¤ì¼€ì¼ë§ (ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ë¬¸ì œ ë°©ì§€(vanishing/exploding))\n",
    "    score = score / sqrt(self.d_model)\n",
    "\n",
    "    ### 4. ë§ˆìŠ¤í‚¹ â€“ í•„ìš” ì‹œ íŒ¨ë”©ì´ë‚˜ íŠ¹ì • í† í°ì„ ì œì™¸í•˜ê¸° ìœ„í•´ ë§ˆìŠ¤í¬ë¥¼ ì ìš©\n",
    "    if mask is not None:\n",
    "      score = score + (mask * -1e9) # (ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜(ê°’ 1))ì— ë§¤ìš° í° ìŒìˆ˜ ì¶”ê°€ -> ì†Œí”„íŠ¸ ë§¥ìŠ¤ì—ì„œ 0ì´ë¨)\n",
    "\n",
    "    ### 5. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    score = self.softmax(score)\n",
    "\n",
    "\t### 6. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì ìš© â€“ ê°’ ë²¡í„°(V)ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ìµœì¢… ê²°ê³¼ ìƒì„±\n",
    "    # WvëŠ” ìœ ì‚¬ë„ê°€ ë†’ì€ ë‹¨ì–´ë¡œë¶€í„° 'ì˜¬ë°”ë¥¸ ì •ë³´'ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•™ìŠµëœë‹¤.\n",
    "    result = torch.matmul(score, v) # ê°€ì¤‘ì¹˜ í•©ì´ ì ìš©ë˜ì–´ ì»¥í…ìŠ¤íŠ¸ ë²¡í„°ê°€ ëœë‹¤.\n",
    "    # ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ëŠ” ê° ë‹¨ì–´(ë°¸ë¥˜ ë²¡í„°)ì— ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ê³±í•œ ê°€ì¤‘í•©ì…ë‹ˆë‹¤.\n",
    "    # C_3 = A_<3, 1>V3 + A_<3, 2>V2 + A<3, 3>V3 + A<3, 4>V4\n",
    "\n",
    "\n",
    "    ### 7. ì¶œë ¥ ë³€í™˜ â€“ ì„ í˜• ë³€í™˜(dense)ì„ í†µí•´ ìµœì¢… ì¶œë ¥ ìƒì„±\n",
    "    result = self.dense(result)\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [MY CODE] Multi-Head-Attention (MHA) êµ¬í˜„"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ë©€í‹° í—¤ë“œ ì–´í…ì…˜\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, n_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim # ì…ë ¥ ì°¨ì›  (ë³´í†µ ì–´íœ˜ í¬ê¸°((vocab_size)))\n",
    "    self.d_model = d_model     # ì„ë² ë”© ì°¨ì› (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥´ë‹¤. BERTëŠ” 768..)\n",
    "    self.n_heads = n_heads      # í—¤ë“œ ìˆ˜ (ë©€í‹° í—¤ë“œ)\n",
    "    assert d_model % n_heads == 0, \"d_modelì€ n_headsë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    # í—¤ë“œëŠ” ë…ë¦½ì ‘ìœ¼ë¡œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” í•˜ë‚˜ì˜ ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì´ë‹¤.\n",
    "\n",
    "    # ê° í—¤ë“œì˜ ì°¨ì›\n",
    "    self.depth = d_model // n_heads # (D' = d_model / H)\n",
    "\n",
    "    self.wq = nn.Linear(input_dim, d_model)  # ì¿¼ë¦¬(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wk = nn.Linear(input_dim, d_model)  #   í‚¤(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wv = nn.Linear(input_dim, d_model)  #   ê°’(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "\n",
    "    # ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ (d_model -> d_model)\n",
    "    self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # ì†Œí”„íŠ¸ë§¥ìŠ¤ (ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•´((ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ìˆ˜í–‰)))\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    Q, K, Vë¥¼ í—¤ë“œ ìˆ˜(n_heads)ë¡œ ë‚˜ëˆ„ê³  transposeí•˜ì—¬ (B, H, S, D) í˜•íƒœë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    x = x.view(batch_size, -1, self.n_heads, self.depth)\n",
    "    return x.transpose(1, 2)  # (B, S, H, D') -> (B, H, S, D')\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # ğŸ”¹ 1. Q, K, V ë²¡í„° ìƒì„± â€“ ì…ë ¥ì„ í†µí•´ Wq, Wk, Wvë¥¼ ì„ í˜• ë³€í™˜í•˜ì—¬ Q, K, Vë¥¼ ë§Œë“¬\n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "    # Q (Query): í˜„ì¬ ê¸°ì¤€ì´ ë˜ëŠ” í† í° (ê²€ìƒ‰ ê¸°ì¤€)\n",
    "    # K (Key): ëª¨ë“  í† í° (ê¸°ì¤€ê³¼ ë¹„êµí•  ëŒ€ìƒ)\n",
    "    # V (Value): ìµœì¢…ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ ë°˜í™˜í•  ê°’\n",
    "    # ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì€ Qì™€ Kì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ Vì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì–¼ë§ˆë§Œí¼ ê°€ì ¸ì˜¬ì§€ ê²°ì •í•œë‹¤.\n",
    "\n",
    "    # ğŸ”¹ 2. í—¤ë“œë¡œ ë‚˜ëˆˆë‹¤.\n",
    "    batch_size = x.shape[0]\n",
    "    q = self.split_heads(q, batch_size)  # (B, H, S, D)\n",
    "    k = self.split_heads(k, batch_size)  # (B, H, S, D)\n",
    "    v = self.split_heads(v, batch_size)  # (B, H, S, D)\n",
    "\n",
    "    # ğŸ”¹ 3. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° â€“ ì¿¼ë¦¬(Q)ì™€ í‚¤(K)ë¥¼ ë‚´ì í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° (Q * K^T)\n",
    "    # W_qì™€ W_këŠ” ë¬¸ë§¥ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ë†’ì€ ë‚´ì  ê°’ì„ ë‚´ë±‰ë„ë¡ í•™ìŠµë‡Œë‹¤.\n",
    "    # B(Batch Size: ë°°ì¹˜ í¬ê¸°), S(Sequence length: ì‹œí€€ìŠ¤ ê¸¸ì´), D(Embedding Dimension: ì„ë² ë”© ì°¨ì›)\n",
    "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "    # ì–´í…ì…˜ì—ì„œ ë‚´ì  ê²°ê³¼ë¥¼ ì—ë„ˆì§€ë¼ê³ ë„ í•œë‹¤.\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ í¬ë©´: ì¿¼ë¦¬ì™€ í‚¤ê°€ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ ì‘ì„ìˆ˜ë¡ ë‘ ë²¡í„°ê°„ì˜ ìƒê´€ì„±ì´ ë‚®ë‹¤ëŠ” ì˜ë¯¸\n",
    "\n",
    "    # ğŸ”¹ 4. ìŠ¤ì½”ì–´ ì •ê·œí™” â€“ sqrt(d_model)ë¡œ ë‚˜ëˆ ì„œ ìŠ¤ì¼€ì¼ë§ (ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ë¬¸ì œ ë°©ì§€(vanishing/exploding))\n",
    "    # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œëŠ” ì„ë² ë”© ì°¨ì›ì´ ì‘ì•„ì¡Œìœ¼ë¯€ë¡œ depthë¡œ ë‚˜ë„ˆì•¼ í•¨\n",
    "    score = score / sqrt(self.depth)\n",
    "\n",
    "    # ğŸ”¹ 5. ë§ˆìŠ¤í‚¹ â€“ í•„ìš” ì‹œ íŒ¨ë”©ì´ë‚˜ íŠ¹ì • í† í°ì„ ì œì™¸í•˜ê¸° ìœ„í•´ ë§ˆìŠ¤í¬ë¥¼ ì ìš©\n",
    "    if mask is not None:\n",
    "      score = score + (mask * -1e9) # (ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜(ê°’ 1))ì— ë§¤ìš° í° ìŒìˆ˜ ì¶”ê°€ -> ì†Œí”„íŠ¸ ë§¥ìŠ¤ì—ì„œ 0ì´ë¨)\n",
    "\n",
    "    # ğŸ”¹ 6. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    score = self.softmax(score)\n",
    "\n",
    "    # ğŸ”¹ 7. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì ìš© â€“ ê°’ ë²¡í„°(V)ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ìµœì¢… ê²°ê³¼ ìƒì„±\n",
    "    # WvëŠ” ìœ ì‚¬ë„ê°€ ë†’ì€ ë‹¨ì–´ë¡œë¶€í„° 'ì˜¬ë°”ë¥¸ ì •ë³´'ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•™ìŠµëœë‹¤.\n",
    "    result = torch.matmul(score, v) # ê°€ì¤‘ì¹˜ í•©ì´ ì ìš©ë˜ì–´ ì»¥í…ìŠ¤íŠ¸ ë²¡í„°ê°€ ëœë‹¤.\n",
    "    # ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ëŠ” ê° ë‹¨ì–´(ë°¸ë¥˜ ë²¡í„°)ì— ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ê³±í•œ ê°€ì¤‘í•©ì…ë‹ˆë‹¤.\n",
    "    # C_3 = A_<3, 1>V3 + A_<3, 2>V2 + A<3, 3>V3 + A<3, 4>V4\n",
    "\n",
    "    \"\"\"\n",
    "    ì…ë ¥ ì„ë² ë”© (B, S, 512)\n",
    "    â””â”€â”€ (512ì°¨ì›ì„ 8ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆ”)\n",
    "          â”œâ”€â”€ 1ë²ˆì§¸ í—¤ë“œ (64ì°¨ì›)\n",
    "          â”œâ”€â”€ 2ë²ˆì§¸ í—¤ë“œ (64ì°¨ì›)\n",
    "          â”œâ”€â”€ ...\n",
    "          â””â”€â”€ 8ë²ˆì§¸ í—¤ë“œ (64ì°¨ì›)\n",
    "    â””â”€â”€ (ë‹¤ì‹œ ë³‘í•©í•˜ì—¬ 512ì°¨ì›ìœ¼ë¡œ ë³µì›)\n",
    "    \"\"\"\n",
    "    # ğŸ”¹ 8. í—¤ë“œ í•©ì¹˜ê¸°\n",
    "    # ì°¨ì› ì¦ê°€ê°€ ì•„ë‹ˆë¼, ì¤‘ë³µ í•©ì¹˜ê¸°ê°€ ì•„ë‹ˆë¼ ë‹¨ìˆœíˆ ë” ì‘ê²Œ ë‚˜ëˆ„ì—ˆë˜ ì°¨ì›ì„ ë‹¤ì‹œ í•©ì¹˜ëŠ” ê³¼ì •\n",
    "    result = result.transpose(1, 2).contiguous()  # (B, S, H, D)\n",
    "    result = result.view(batch_size, -1, self.d_model)  # (B, S, D)\n",
    "\n",
    "    # ğŸ”¹ 9. ì¶œë ¥ ë³€í™˜ â€“ ì„ í˜• ë³€í™˜(dense)ì„ í†µí•´ ìµœì¢… ì¶œë ¥ ìƒì„±\n",
    "    result = self.dense(result)\n",
    "\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VYrqTJagS1"
   },
   "source": [
    "## Positional encoding (ì£¼ì„ ì¶”ê°€)\n",
    "\n",
    "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ìˆœì°¨ì ì¸ RNNê³¼ ë‹¬ë¦¬ ìœ„ì¹˜ ì •ë³´ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜ë˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ì´ í•„ìš”í•˜ë‹¤.\n",
    "- ì„ë² ë”© ì°¨ì›ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” ì‚¬ì¸(Sin)\n",
    "- í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” ì½”ì‚¬ì¸(Cos)ì„ ì ìš©í•´ ë‹¨ì–´ ìœ„ì¹˜ì— ë”°ë¼ ê·œì¹™ì ì´ê³  ì£¼ê¸°ì ì¸ íŒ¨í„´ì„ ì ìš©í•œë‹¤.\n",
    "ì¥ì :\n",
    "-> -1 ~ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë„ˆë¬´ í¬ì§€ ì•Šë‹¤.\n",
    "-> ì„œë¡œ ë‹¤ë¥¸ ì£¼ê¸°ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì¤‘ë³µë˜ì§€ ì•Šë‹¤. (Sin(1)ê³¼ Sin(2)ëŠ” ë§¤ìš° ë‹¤ë¥¸ ê°’ì„ ê°€ì§„ë‹¤.)\n",
    "\n",
    "$$\n",
    "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
    "$$\n",
    "\n",
    "ì´ë¥¼ Numpyë¡œ êµ¬í˜„í•˜ì—¬ PyTorch tensorë¡œ ë³€í™˜í•œ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1723896343031,
     "user": {
      "displayName": "ì¡°ìŠ¹í˜",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Uf_jMQWDUR79",
    "outputId": "534712be-1522-4d32-81b7-87f50a6f1f2a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"\n",
    "    pos: ìœ„ì¹˜ (Position) â†’ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ (ë°°ì¹˜ì˜ ë‹¨ì–´ ì¸ë±ìŠ¤)\n",
    "    i: ì„ë² ë”© ì°¨ì›ì˜ ì¸ë±ìŠ¤ (0, 1, 2, ...)\n",
    "    d_model: ì „ì²´ ì„ë² ë”© ì°¨ì› (ì˜ˆ: 256, 512 ë“±)\n",
    "\n",
    "    ê° ìœ„ì¹˜(pos)ì™€ ì„ë² ë”© ì°¨ì›(i)ì— ë”°ë¼ ê³ ìœ í•œ ê°ë„ ê°’(angle_rates)ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    ì´ ê°ë„ëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³ ë„ ë„¤íŠ¸ì›Œí¬ê°€ ìˆœì„œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ\n",
    "    \"\"\"\n",
    "\n",
    "    # ì£¼ì–´ì§„ ìœ„ì¹˜ì™€ ì„ë² ë”© ì°¨ì›ì— ë”°ë¼ ê°ë„ ë³€í™”ìœ¨(angle_rates) ê³„ì‚°\n",
    "    # 10000ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ì¤€ ìƒìˆ˜ë¡œ, ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ë§ ì—­í• ì„ í•¨\n",
    "    # iê°€ ì»¤ì§ˆìˆ˜ë¡(ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡) ê°ë„ ë³€í™”ìœ¨ì´ ì‘ì•„ì§\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    position: ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ (ì‹œí€€ìŠ¤ ê¸¸ì´, ì˜ˆ: 400)\n",
    "    d_model: ì„ë² ë”© ì°¨ì› (ì˜ˆ: 256)\n",
    "\n",
    "    ê° ìœ„ì¹˜ì— ëŒ€í•œ ì‚¬ì¸(Sin)ê³¼ ì½”ì‚¬ì¸(Cosine)ì„ ê¸°ë°˜ìœ¼ë¡œ í¬ì§€ì…”ë„ ì¸ì½”ë”© í–‰ë ¬ì„ ìƒì„±\n",
    "    \"\"\"\n",
    "    # ê° ìœ„ì¹˜(pos)ì™€ ì„ë² ë”© ì°¨ì›(i)ì— ëŒ€í•´ get_angles í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ê°ë„ ê³„ì‚°\n",
    "    # np.arange(position)[:, None]: ìœ„ì¹˜ ì¸ë±ìŠ¤ (ì„¸ë¡œë¡œ í™•ì¥) [[0] [1] [2] [3] [4] ...]\n",
    "    # np.arange(d_model)[None, :]: ì„ë² ë”© ì°¨ì› ì¸ë±ìŠ¤ (ê°€ë¡œë¡œ í™•ì¥) [[0 1 2 3 4 ...]]\n",
    "    # ê²°ê³¼ì ìœ¼ë¡œ (position, d_model) í¬ê¸°ì˜ ê°ë„ í–‰ë ¬(angle_rads)ì„ ìƒì„±\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    # ì§ìˆ˜ ì¸ë±ìŠ¤(0, 2, 4...) â†’ ì‚¬ì¸(Sin) ì ìš©\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # í™€ìˆ˜ ì¸ë±ìŠ¤(1, 3, 5...) â†’ ì½”ì‚¬ì¸(Cos) ì ìš©\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # Positional Encodingì„ 3ì°¨ì›ìœ¼ë¡œ í™•ì¥ (Batch ì°¨ì› ì¶”ê°€)\n",
    "    # (1, position, d_model) í˜•íƒœ â†’ ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•´ ì—¬ëŸ¬ ë¬¸ì¥ì— ì ìš© ê°€ëŠ¥\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "\n",
    "max_len = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [MY CODE] TransformerLayer - Layer normalization, dropout, residual connection êµ¬í˜„"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, dff, n_heads, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.d_model = d_model\n",
    "    self.dff = dff\n",
    "\n",
    "    # ğŸ”¹ ê¸°ì¡´ Self-Attentionì„ MultiHeadAttentionìœ¼ë¡œ ë³€ê²½\n",
    "    self.mha = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "\n",
    "    # ğŸ”¹ Layer Normalization ì¶”ê°€\n",
    "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # ğŸ”¹ Dropout ì¶”ê°€\n",
    "    self.dropout1 = nn.Dropout(dropout_rate)\n",
    "    self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    # ğŸ”¹ Feed Forward Network (FFN)\n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(d_model, dff),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dff, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # ğŸ”¹ Multi-Head Attention + Dropout + Residual Connection + LayerNorm\n",
    "    x1 = self.mha(x, mask)\n",
    "    x1 = self.dropout1(x1)\n",
    "    x1 = self.layer_norm1(x1 + x)\n",
    "\n",
    "    # ğŸ”¹ Feed Forward Network + Dropout + Residual Connection + LayerNorm\n",
    "    x2 = self.ffn(x1)\n",
    "    x2 = self.dropout2(x2)\n",
    "    x2 = self.layer_norm2(x2 + x1)\n",
    "\n",
    "    return x2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MaiCGh8TsDH"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
    "    super().__init__()\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.d_model = d_model\n",
    "    self.n_layers = n_layers\n",
    "    self.dff = dff\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
    "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
    "    self.classification = nn.Linear(d_model, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    mask = (x == tokenizer.pad_token_id)\n",
    "    mask = mask[:, None, :]\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    x = x * sqrt(self.d_model)\n",
    "    x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, mask)\n",
    "\n",
    "    x = x[:, 0]\n",
    "    x = self.classification(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = TextClassifier(len(tokenizer), 32, 2, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXpjPWHjbUK8"
   },
   "source": [
    "ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ì ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "1. `nn.ModuleList`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ layerì˜ êµ¬í˜„ì„ ì‰½ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "2. Embedding, positional encoding, transformer layerë¥¼ ê±°ì¹˜ê³  ë‚œ í›„ ë§ˆì§€ë§‰ labelì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œ ê°’ì€ `x[:, 0]`ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ RNNì—ì„œëŠ” padding tokenì„ ì œì™¸í•œ ë§ˆì§€ë§‰ tokenì— í•´ë‹¹í•˜ëŠ” representationì„ ì‚¬ìš©í•œ ê²ƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì´ë ‡ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” attention ê³¼ì •ì„ ë³´ì‹œë©´ ì²« ë²ˆì§¸ tokenì— ëŒ€í•œ representationì€ ì´í›„ì˜ ëª¨ë“  tokenì˜ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤. ì¦‰, ì²« ë²ˆì§¸ token ë˜í•œ ì „ì²´ ë¬¸ì¥ì„ ëŒ€ë³€í•˜ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì¼ë°˜ì ìœ¼ë¡œ Transformerë¥¼ text ë¶„ë¥˜ì— ì‚¬ìš©í•  ë•ŒëŠ” ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDq05OlAb2lB"
   },
   "source": [
    "## í•™ìŠµ\n",
    "\n",
    "í•™ìŠµí•˜ëŠ” ì½”ë“œëŠ” ê¸°ì¡´ ì‹¤ìŠµë“¤ê³¼ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ ê²°ê³¼ë§Œ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHVVsWBPQmnv",
    "outputId": "64b5790f-7649-4a47-95f8-bebe158aba4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to('cuda')\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r88BALxO1zc1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "    preds = model(inputs)\n",
    "    # preds = torch.argmax(preds, dim=-1)\n",
    "    preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 408929,
     "status": "ok",
     "timestamp": 1723896769492,
     "user": {
      "displayName": "ì¡°ìŠ¹í˜",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "al_b56TYRILq",
    "outputId": "90a56264-4ef3-4def-e7b7-df4b5cd3c305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 228.05881744623184\n",
      "=========> Train acc: 0.770 | Test acc: 0.745\n",
      "Epoch   1 | Train Loss: 181.79184779524803\n",
      "=========> Train acc: 0.818 | Test acc: 0.776\n",
      "Epoch   2 | Train Loss: 161.0439212024212\n",
      "=========> Train acc: 0.847 | Test acc: 0.786\n",
      "Epoch   3 | Train Loss: 144.77784506976604\n",
      "=========> Train acc: 0.866 | Test acc: 0.798\n",
      "Epoch   4 | Train Loss: 127.65684458613396\n",
      "=========> Train acc: 0.892 | Test acc: 0.800\n",
      "Epoch   5 | Train Loss: 113.74214302748442\n",
      "=========> Train acc: 0.907 | Test acc: 0.804\n",
      "Epoch   6 | Train Loss: 101.79052671045065\n",
      "=========> Train acc: 0.916 | Test acc: 0.799\n",
      "Epoch   7 | Train Loss: 90.75403520092368\n",
      "=========> Train acc: 0.937 | Test acc: 0.809\n",
      "Epoch   8 | Train Loss: 77.53205958008766\n",
      "=========> Train acc: 0.945 | Test acc: 0.808\n",
      "Epoch   9 | Train Loss: 68.24311332032084\n",
      "=========> Train acc: 0.957 | Test acc: 0.804\n",
      "Epoch  10 | Train Loss: 58.22225522249937\n",
      "=========> Train acc: 0.964 | Test acc: 0.801\n",
      "Epoch  11 | Train Loss: 51.82508478872478\n",
      "=========> Train acc: 0.964 | Test acc: 0.798\n",
      "Epoch  12 | Train Loss: 44.73537188582122\n",
      "=========> Train acc: 0.975 | Test acc: 0.800\n",
      "Epoch  13 | Train Loss: 36.328937944956124\n",
      "=========> Train acc: 0.970 | Test acc: 0.795\n",
      "Epoch  14 | Train Loss: 31.212427048478276\n",
      "=========> Train acc: 0.984 | Test acc: 0.799\n",
      "Epoch  15 | Train Loss: 27.794022045098245\n",
      "=========> Train acc: 0.986 | Test acc: 0.798\n",
      "Epoch  16 | Train Loss: 24.51000529155135\n",
      "=========> Train acc: 0.988 | Test acc: 0.798\n",
      "Epoch  17 | Train Loss: 25.019716920796782\n",
      "=========> Train acc: 0.988 | Test acc: 0.795\n",
      "Epoch  18 | Train Loss: 18.76576793473214\n",
      "=========> Train acc: 0.989 | Test acc: 0.794\n",
      "Epoch  19 | Train Loss: 19.29518177837599\n",
      "=========> Train acc: 0.992 | Test acc: 0.798\n",
      "Epoch  20 | Train Loss: 16.6582816374721\n",
      "=========> Train acc: 0.992 | Test acc: 0.802\n",
      "Epoch  21 | Train Loss: 17.27638038888108\n",
      "=========> Train acc: 0.991 | Test acc: 0.791\n",
      "Epoch  22 | Train Loss: 15.25645094725769\n",
      "=========> Train acc: 0.989 | Test acc: 0.797\n",
      "Epoch  23 | Train Loss: 14.676565335481428\n",
      "=========> Train acc: 0.995 | Test acc: 0.795\n",
      "Epoch  24 | Train Loss: 11.922925418621162\n",
      "=========> Train acc: 0.994 | Test acc: 0.792\n",
      "Epoch  25 | Train Loss: 12.693496667896397\n",
      "=========> Train acc: 0.988 | Test acc: 0.797\n",
      "Epoch  26 | Train Loss: 11.326061181141995\n",
      "=========> Train acc: 0.972 | Test acc: 0.789\n",
      "Epoch  27 | Train Loss: 12.779942307330202\n",
      "=========> Train acc: 0.996 | Test acc: 0.795\n",
      "Epoch  28 | Train Loss: 10.617993225852842\n",
      "=========> Train acc: 0.993 | Test acc: 0.800\n",
      "Epoch  29 | Train Loss: 9.673813316738233\n",
      "=========> Train acc: 0.996 | Test acc: 0.794\n",
      "Epoch  30 | Train Loss: 11.522750682837795\n",
      "=========> Train acc: 0.996 | Test acc: 0.794\n",
      "Epoch  31 | Train Loss: 7.840653722785646\n",
      "=========> Train acc: 0.994 | Test acc: 0.792\n",
      "Epoch  32 | Train Loss: 12.104161699942779\n",
      "=========> Train acc: 0.995 | Test acc: 0.793\n",
      "Epoch  33 | Train Loss: 8.25330251167179\n",
      "=========> Train acc: 0.985 | Test acc: 0.793\n",
      "Epoch  34 | Train Loss: 10.357778827659786\n",
      "=========> Train acc: 0.997 | Test acc: 0.796\n",
      "Epoch  35 | Train Loss: 9.444769158624695\n",
      "=========> Train acc: 0.995 | Test acc: 0.794\n",
      "Epoch  36 | Train Loss: 8.166781437234022\n",
      "=========> Train acc: 0.996 | Test acc: 0.792\n",
      "Epoch  37 | Train Loss: 9.650583731883671\n",
      "=========> Train acc: 0.997 | Test acc: 0.795\n",
      "Epoch  38 | Train Loss: 7.418147705044248\n",
      "=========> Train acc: 0.996 | Test acc: 0.794\n",
      "Epoch  39 | Train Loss: 9.709464895306155\n",
      "=========> Train acc: 0.997 | Test acc: 0.794\n",
      "Epoch  40 | Train Loss: 7.829162544643623\n",
      "=========> Train acc: 0.996 | Test acc: 0.792\n",
      "Epoch  41 | Train Loss: 6.924684327503201\n",
      "=========> Train acc: 0.995 | Test acc: 0.786\n",
      "Epoch  42 | Train Loss: 7.91478517052019\n",
      "=========> Train acc: 0.996 | Test acc: 0.792\n",
      "Epoch  43 | Train Loss: 7.378424593654927\n",
      "=========> Train acc: 0.998 | Test acc: 0.793\n",
      "Epoch  44 | Train Loss: 6.822792663617292\n",
      "=========> Train acc: 0.996 | Test acc: 0.795\n",
      "Epoch  45 | Train Loss: 6.635453788730956\n",
      "=========> Train acc: 0.998 | Test acc: 0.795\n",
      "Epoch  46 | Train Loss: 6.294900413195137\n",
      "=========> Train acc: 0.996 | Test acc: 0.798\n",
      "Epoch  47 | Train Loss: 7.062853217605152\n",
      "=========> Train acc: 0.994 | Test acc: 0.793\n",
      "Epoch  48 | Train Loss: 6.103620611334918\n",
      "=========> Train acc: 0.998 | Test acc: 0.797\n",
      "Epoch  49 | Train Loss: 5.912332823994802\n",
      "=========> Train acc: 0.995 | Test acc: 0.794\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for data in train_loader:\n",
    "    model.zero_grad()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n",
    "\n",
    "    preds = model(inputs)[..., 0]\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqZays2yb8Ja"
   },
   "source": [
    "í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë˜ë©° RNNë³´ë‹¤ ë¹¨ë¦¬ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "í•˜ì§€ë§Œ test ì •í™•ë„ê°€ RNNë³´ë‹¤ ë‚®ì€ ê²ƒì„ ë³´ì•˜ì„ ë•Œ, overfittingì— ì·¨ì•½í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAXB6GgIQy1S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.]],\n",
      "\n",
      "        [[2., 4., 1.],\n",
      "         [5., 3., 1.]]])\n",
      "tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.]],\n",
      "\n",
      "        [[2., 4., 1.],\n",
      "         [5., 3., 1.]]])\n",
      "tensor([[[0.2689, 0.1192, 0.8808],\n",
      "         [0.2689, 0.8808, 0.9933]],\n",
      "\n",
      "        [[0.7311, 0.8808, 0.1192],\n",
      "         [0.7311, 0.1192, 0.0067]]])\n",
      "tensor([[[0.0900, 0.2447, 0.6652],\n",
      "         [0.0900, 0.2447, 0.6652]],\n",
      "\n",
      "        [[0.1142, 0.8438, 0.0420],\n",
      "         [0.8668, 0.1173, 0.0159]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "softmax1 = nn.Softmax()  #\n",
    "softmax2 = nn.Softmax(dim=-1)  # ë§ˆì§€ë§‰ ì°¨ì›ì— ì ìš©\n",
    "x = torch.tensor([[[1.0, 2.0, 3.0],\n",
    "                   [4.0, 5.0, 6.0]],\n",
    "\n",
    "                  [[2.0, 4.0, 1.0],\n",
    "                   [5.0, 3.0, 1.0]]])\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n",
    "output1 = softmax1(x)\n",
    "output2 = softmax2(x)\n",
    "print(x)\n",
    "print(output1)\n",
    "print(output2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
