{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymxatB5WYxlL"
   },
   "source": [
    "# Transformer ì‹¤ìŠµ\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ê°ì • ë¶„ì„ taskì— RNN ëŒ€ì‹  Transformerë¥¼ êµ¬í˜„í•˜ì—¬ ì ìš©í•´ ë³¼ ê²ƒì…ë‹ˆë‹¤.\n",
    "Library importë‚˜ dataloader ìƒì„±ì€ RNN ì‹¤ìŠµ ë•Œì™€ ë˜‘ê°™ê¸° ë•Œë¬¸ì— ì„¤ëª…ì€ ë„˜ì–´ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HOdhoBVA1zcu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimhongil/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(row['label'])\n",
    "    texts.append(row['text'])\n",
    "\n",
    "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-FshZcTZBQ2"
   },
   "source": [
    "## Self-Attention (ì£¼ì„ ì¶”ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MBlMVMZcRAxv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "# ì…€í”„ ì–´í…ì…˜\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim # ì…ë ¥ ì°¨ì›  (ë³´í†µ ì–´íœ˜ í¬ê¸°((vocab_size)))\n",
    "    self.d_model = d_model     # ì„ë² ë”© ì°¨ì› (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥´ë‹¤. BERTëŠ” 768..)\n",
    "\n",
    "    self.wq = nn.Linear(input_dim, d_model)  # ì¿¼ë¦¬(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wk = nn.Linear(input_dim, d_model)  #   í‚¤(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wv = nn.Linear(input_dim, d_model)  #   ê°’(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "\n",
    "    # ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ (d_model -> d_model)\n",
    "    self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # ì†Œí”„íŠ¸ë§¥ìŠ¤ (ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•´((ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ìˆ˜í–‰)))\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    ### 1. Q, K, V ë²¡í„° ìƒì„± â€“ ì…ë ¥ì„ í†µí•´ Wq, Wk, Wvë¥¼ ì„ í˜• ë³€í™˜í•˜ì—¬ Q, K, Vë¥¼ ë§Œë“¬\n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\t# Q (Query): í˜„ì¬ ê¸°ì¤€ì´ ë˜ëŠ” í† í° (ê²€ìƒ‰ ê¸°ì¤€)\n",
    "    # K (Key): ëª¨ë“  í† í° (ê¸°ì¤€ê³¼ ë¹„êµí•  ëŒ€ìƒ)\n",
    "    # V (Value): ìµœì¢…ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ ë°˜í™˜í•  ê°’\n",
    "    # ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì€ Qì™€ Kì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ Vì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì–¼ë§ˆë§Œí¼ ê°€ì ¸ì˜¬ì§€ ê²°ì •í•œë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "    ### 2. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° â€“ ì¿¼ë¦¬(Q)ì™€ í‚¤(K)ë¥¼ ë‚´ì í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° (Q * K^T)\n",
    "    # W_qì™€ W_këŠ” ë¬¸ë§¥ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ë†’ì€ ë‚´ì  ê°’ì„ ë‚´ë±‰ë„ë¡ í•™ìŠµë‡Œë‹¤.\n",
    "    # B(Batch Size: ë°°ì¹˜ í¬ê¸°), S(Sequence length: ì‹œí€€ìŠ¤ ê¸¸ì´), D(Embedding Dimension: ì„ë² ë”© ì°¨ì›)\n",
    "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "    # ì–´í…ì…˜ì—ì„œ ë‚´ì  ê²°ê³¼ë¥¼ ì—ë„ˆì§€ë¼ê³ ë„ í•œë‹¤.\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ í¬ë©´: ì¿¼ë¦¬ì™€ í‚¤ê°€ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ ì‘ì„ìˆ˜ë¡ ë‘ ë²¡í„°ê°„ì˜ ìƒê´€ì„±ì´ ë‚®ë‹¤ëŠ” ì˜ë¯¸\n",
    "\n",
    "    ### 3. ìŠ¤ì½”ì–´ ì •ê·œí™” â€“ sqrt(d_model)ë¡œ ë‚˜ëˆ ì„œ ìŠ¤ì¼€ì¼ë§ (ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ë¬¸ì œ ë°©ì§€(vanishing/exploding))\n",
    "    score = score / sqrt(self.d_model)\n",
    "\n",
    "    ### 4. ë§ˆìŠ¤í‚¹ â€“ í•„ìš” ì‹œ íŒ¨ë”©ì´ë‚˜ íŠ¹ì • í† í°ì„ ì œì™¸í•˜ê¸° ìœ„í•´ ë§ˆìŠ¤í¬ë¥¼ ì ìš©\n",
    "    if mask is not None:\n",
    "      score = score + (mask * -1e9) # (ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜(ê°’ 1))ì— ë§¤ìš° í° ìŒìˆ˜ ì¶”ê°€ -> ì†Œí”„íŠ¸ ë§¥ìŠ¤ì—ì„œ 0ì´ë¨)\n",
    "\n",
    "    ### 5. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    score = self.softmax(score)\n",
    "\n",
    "\t### 6. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì ìš© â€“ ê°’ ë²¡í„°(V)ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ìµœì¢… ê²°ê³¼ ìƒì„±\n",
    "    # WvëŠ” ìœ ì‚¬ë„ê°€ ë†’ì€ ë‹¨ì–´ë¡œë¶€í„° 'ì˜¬ë°”ë¥¸ ì •ë³´'ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•™ìŠµëœë‹¤.\n",
    "    result = torch.matmul(score, v) # ê°€ì¤‘ì¹˜ í•©ì´ ì ìš©ë˜ì–´ ì»¥í…ìŠ¤íŠ¸ ë²¡í„°ê°€ ëœë‹¤.\n",
    "    # ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ëŠ” ê° ë‹¨ì–´(ë°¸ë¥˜ ë²¡í„°)ì— ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ê³±í•œ ê°€ì¤‘í•©ì…ë‹ˆë‹¤.\n",
    "    # C_3 = A_<3, 1>V3 + A_<3, 2>V2 + A<3, 3>V3 + A<3, 4>V4\n",
    "\n",
    "\n",
    "    ### 7. ì¶œë ¥ ë³€í™˜ â€“ ì„ í˜• ë³€í™˜(dense)ì„ í†µí•´ ìµœì¢… ì¶œë ¥ ìƒì„±\n",
    "    result = self.dense(result)\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [MY CODE] Multi-Head-Attention (MHA) êµ¬í˜„"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# ë©€í‹° í—¤ë“œ ì–´í…ì…˜\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, n_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim # ì…ë ¥ ì°¨ì›  (ë³´í†µ ì–´íœ˜ í¬ê¸°((vocab_size)))\n",
    "    self.d_model = d_model     # ì„ë² ë”© ì°¨ì› (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥´ë‹¤. BERTëŠ” 768..)\n",
    "    self.n_heads = n_heads      # í—¤ë“œ ìˆ˜ (ë©€í‹° í—¤ë“œ)\n",
    "    assert d_model % n_heads == 0, \"d_modelì€ n_headsë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    # í—¤ë“œëŠ” ë…ë¦½ì ‘ìœ¼ë¡œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” í•˜ë‚˜ì˜ ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì´ë‹¤.\n",
    "\n",
    "    # ê° í—¤ë“œì˜ ì°¨ì›\n",
    "    self.depth = d_model // n_heads # (D' = d_model / H)\n",
    "\n",
    "    self.wq = nn.Linear(input_dim, d_model)  # ì¿¼ë¦¬(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wk = nn.Linear(input_dim, d_model)  #   í‚¤(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "    self.wv = nn.Linear(input_dim, d_model)  #   ê°’(ì…ë ¥ -> d_model ì°¨ì› ë³€í™˜)\n",
    "\n",
    "    # ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ (d_model -> d_model)\n",
    "    self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # ì†Œí”„íŠ¸ë§¥ìŠ¤ (ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•´((ë§ˆì§€ë§‰ ì°¨ì› ê¸°ì¤€ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ìˆ˜í–‰)))\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    Q, K, Vë¥¼ í—¤ë“œ ìˆ˜(n_heads)ë¡œ ë‚˜ëˆ„ê³  transposeí•˜ì—¬ (B, H, S, D) í˜•íƒœë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    x = x.view(batch_size, -1, self.n_heads, self.depth)\n",
    "    return x.transpose(1, 2)  # (B, S, H, D') -> (B, H, S, D')\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # ğŸ”¹ 1. Q, K, V ë²¡í„° ìƒì„± â€“ ì…ë ¥ì„ í†µí•´ Wq, Wk, Wvë¥¼ ì„ í˜• ë³€í™˜í•˜ì—¬ Q, K, Vë¥¼ ë§Œë“¬\n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "    # Q (Query): í˜„ì¬ ê¸°ì¤€ì´ ë˜ëŠ” í† í° (ê²€ìƒ‰ ê¸°ì¤€)\n",
    "    # K (Key): ëª¨ë“  í† í° (ê¸°ì¤€ê³¼ ë¹„êµí•  ëŒ€ìƒ)\n",
    "    # V (Value): ìµœì¢…ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ ë°˜í™˜í•  ê°’\n",
    "    # ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜ì€ Qì™€ Kì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ Vì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì–¼ë§ˆë§Œí¼ ê°€ì ¸ì˜¬ì§€ ê²°ì •í•œë‹¤.\n",
    "\n",
    "    # ğŸ”¹ 2. í—¤ë“œë¡œ ë‚˜ëˆˆë‹¤.\n",
    "    batch_size = x.shape[0]\n",
    "    q = self.split_heads(q, batch_size)  # (B, H, S, D)\n",
    "    k = self.split_heads(k, batch_size)  # (B, H, S, D)\n",
    "    v = self.split_heads(v, batch_size)  # (B, H, S, D)\n",
    "\n",
    "    # ğŸ”¹ 3. ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚° â€“ ì¿¼ë¦¬(Q)ì™€ í‚¤(K)ë¥¼ ë‚´ì í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° (Q * K^T)\n",
    "    # W_qì™€ W_këŠ” ë¬¸ë§¥ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ë†’ì€ ë‚´ì  ê°’ì„ ë‚´ë±‰ë„ë¡ í•™ìŠµë‡Œë‹¤.\n",
    "    # B(Batch Size: ë°°ì¹˜ í¬ê¸°), S(Sequence length: ì‹œí€€ìŠ¤ ê¸¸ì´), D(Embedding Dimension: ì„ë² ë”© ì°¨ì›)\n",
    "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "    # ì–´í…ì…˜ì—ì„œ ë‚´ì  ê²°ê³¼ë¥¼ ì—ë„ˆì§€ë¼ê³ ë„ í•œë‹¤.\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ í¬ë©´: ì¿¼ë¦¬ì™€ í‚¤ê°€ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤ëŠ” ì˜ë¯¸\n",
    "    # - ì—ë„ˆì§€ ê°’ì´ ì‘ì„ìˆ˜ë¡ ë‘ ë²¡í„°ê°„ì˜ ìƒê´€ì„±ì´ ë‚®ë‹¤ëŠ” ì˜ë¯¸\n",
    "\n",
    "    # ğŸ”¹ 4. ìŠ¤ì½”ì–´ ì •ê·œí™” â€“ sqrt(d_model)ë¡œ ë‚˜ëˆ ì„œ ìŠ¤ì¼€ì¼ë§ (ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ë¬¸ì œ ë°©ì§€(vanishing/exploding))\n",
    "    # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œëŠ” ì„ë² ë”© ì°¨ì›ì´ ì‘ì•„ì¡Œìœ¼ë¯€ë¡œ depthë¡œ ë‚˜ë„ˆì•¼ í•¨\n",
    "    score = score / sqrt(self.depth)\n",
    "\n",
    "    # ğŸ”¹ 5. ë§ˆìŠ¤í‚¹ â€“ í•„ìš” ì‹œ íŒ¨ë”©ì´ë‚˜ íŠ¹ì • í† í°ì„ ì œì™¸í•˜ê¸° ìœ„í•´ ë§ˆìŠ¤í¬ë¥¼ ì ìš©\n",
    "    if mask is not None:\n",
    "      score = score + (mask * -1e9) # (ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜(ê°’ 1))ì— ë§¤ìš° í° ìŒìˆ˜ ì¶”ê°€ -> ì†Œí”„íŠ¸ ë§¥ìŠ¤ì—ì„œ 0ì´ë¨)\n",
    "\n",
    "    # ğŸ”¹ 6. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "    score = self.softmax(score)\n",
    "\n",
    "    # ğŸ”¹ 7. ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì ìš© â€“ ê°’ ë²¡í„°(V)ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ìµœì¢… ê²°ê³¼ ìƒì„±\n",
    "    # WvëŠ” ìœ ì‚¬ë„ê°€ ë†’ì€ ë‹¨ì–´ë¡œë¶€í„° 'ì˜¬ë°”ë¥¸ ì •ë³´'ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•™ìŠµëœë‹¤.\n",
    "    result = torch.matmul(score, v) # ê°€ì¤‘ì¹˜ í•©ì´ ì ìš©ë˜ì–´ ì»¥í…ìŠ¤íŠ¸ ë²¡í„°ê°€ ëœë‹¤.\n",
    "    # ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ëŠ” ê° ë‹¨ì–´(ë°¸ë¥˜ ë²¡í„°)ì— ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ê³±í•œ ê°€ì¤‘í•©ì…ë‹ˆë‹¤.\n",
    "    # C_3 = A_<3, 1>V3 + A_<3, 2>V2 + A<3, 3>V3 + A<3, 4>V4\n",
    "\n",
    "    \"\"\"\n",
    "    ì…ë ¥ ì„ë² ë”© (B, S, 512)\n",
    "    â””â”€â”€ (512ì°¨ì›ì„ 8ê°œì˜ í—¤ë“œë¡œ ë‚˜ëˆ”)\n",
    "          â”œâ”€â”€ 1ë²ˆì§¸ í—¤ë“œ (64ì°¨ì›)\n",
    "          â”œâ”€â”€ 2ë²ˆì§¸ í—¤ë“œ (64ì°¨ì›)\n",
    "          â”œâ”€â”€ ...\n",
    "          â””â”€â”€ 8ë²ˆì§¸ í—¤ë“œ (64ì°¨ì›)\n",
    "    â””â”€â”€ (ë‹¤ì‹œ ë³‘í•©í•˜ì—¬ 512ì°¨ì›ìœ¼ë¡œ ë³µì›)\n",
    "    \"\"\"\n",
    "    # ğŸ”¹ 8. í—¤ë“œ í•©ì¹˜ê¸°\n",
    "    # ì°¨ì› ì¦ê°€ê°€ ì•„ë‹ˆë¼, ì¤‘ë³µ í•©ì¹˜ê¸°ê°€ ì•„ë‹ˆë¼ ë‹¨ìˆœíˆ ë” ì‘ê²Œ ë‚˜ëˆ„ì—ˆë˜ ì°¨ì›ì„ ë‹¤ì‹œ í•©ì¹˜ëŠ” ê³¼ì •\n",
    "    result = result.transpose(1, 2).contiguous()  # (B, S, H, D)\n",
    "    result = result.view(batch_size, -1, self.d_model)  # (B, S, D)\n",
    "\n",
    "    # ğŸ”¹ 9. ì¶œë ¥ ë³€í™˜ â€“ ì„ í˜• ë³€í™˜(dense)ì„ í†µí•´ ìµœì¢… ì¶œë ¥ ìƒì„±\n",
    "    result = self.dense(result)\n",
    "\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VYrqTJagS1"
   },
   "source": [
    "## Positional encoding (ì£¼ì„ ì¶”ê°€)\n",
    "\n",
    "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ìˆœì°¨ì ì¸ RNNê³¼ ë‹¬ë¦¬ ìœ„ì¹˜ ì •ë³´ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜ë˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ì´ í•„ìš”í•˜ë‹¤.\n",
    "- ì„ë² ë”© ì°¨ì›ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” ì‚¬ì¸(Sin)\n",
    "- í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” ì½”ì‚¬ì¸(Cos)ì„ ì ìš©í•´ ë‹¨ì–´ ìœ„ì¹˜ì— ë”°ë¼ ê·œì¹™ì ì´ê³  ì£¼ê¸°ì ì¸ íŒ¨í„´ì„ ì ìš©í•œë‹¤.\n",
    "ì¥ì :\n",
    "-> -1 ~ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë„ˆë¬´ í¬ì§€ ì•Šë‹¤.\n",
    "-> ì„œë¡œ ë‹¤ë¥¸ ì£¼ê¸°ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì¤‘ë³µë˜ì§€ ì•Šë‹¤. (Sin(1)ê³¼ Sin(2)ëŠ” ë§¤ìš° ë‹¤ë¥¸ ê°’ì„ ê°€ì§„ë‹¤.)\n",
    "\n",
    "$$\n",
    "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
    "$$\n",
    "\n",
    "ì´ë¥¼ Numpyë¡œ êµ¬í˜„í•˜ì—¬ PyTorch tensorë¡œ ë³€í™˜í•œ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1723896343031,
     "user": {
      "displayName": "ì¡°ìŠ¹í˜",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Uf_jMQWDUR79",
    "outputId": "534712be-1522-4d32-81b7-87f50a6f1f2a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"\n",
    "    pos: ìœ„ì¹˜ (Position) â†’ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ (ë°°ì¹˜ì˜ ë‹¨ì–´ ì¸ë±ìŠ¤)\n",
    "    i: ì„ë² ë”© ì°¨ì›ì˜ ì¸ë±ìŠ¤ (0, 1, 2, ...)\n",
    "    d_model: ì „ì²´ ì„ë² ë”© ì°¨ì› (ì˜ˆ: 256, 512 ë“±)\n",
    "\n",
    "    ê° ìœ„ì¹˜(pos)ì™€ ì„ë² ë”© ì°¨ì›(i)ì— ë”°ë¼ ê³ ìœ í•œ ê°ë„ ê°’(angle_rates)ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "    ì´ ê°ë„ëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³ ë„ ë„¤íŠ¸ì›Œí¬ê°€ ìˆœì„œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ\n",
    "    \"\"\"\n",
    "\n",
    "    # ì£¼ì–´ì§„ ìœ„ì¹˜ì™€ ì„ë² ë”© ì°¨ì›ì— ë”°ë¼ ê°ë„ ë³€í™”ìœ¨(angle_rates) ê³„ì‚°\n",
    "    # 10000ì€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ì¤€ ìƒìˆ˜ë¡œ, ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ë§ ì—­í• ì„ í•¨\n",
    "    # iê°€ ì»¤ì§ˆìˆ˜ë¡(ì°¨ì›ì´ ì¦ê°€í• ìˆ˜ë¡) ê°ë„ ë³€í™”ìœ¨ì´ ì‘ì•„ì§\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    position: ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ (ì‹œí€€ìŠ¤ ê¸¸ì´, ì˜ˆ: 400)\n",
    "    d_model: ì„ë² ë”© ì°¨ì› (ì˜ˆ: 256)\n",
    "\n",
    "    ê° ìœ„ì¹˜ì— ëŒ€í•œ ì‚¬ì¸(Sin)ê³¼ ì½”ì‚¬ì¸(Cosine)ì„ ê¸°ë°˜ìœ¼ë¡œ í¬ì§€ì…”ë„ ì¸ì½”ë”© í–‰ë ¬ì„ ìƒì„±\n",
    "    \"\"\"\n",
    "    # ê° ìœ„ì¹˜(pos)ì™€ ì„ë² ë”© ì°¨ì›(i)ì— ëŒ€í•´ get_angles í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ê°ë„ ê³„ì‚°\n",
    "    # np.arange(position)[:, None]: ìœ„ì¹˜ ì¸ë±ìŠ¤ (ì„¸ë¡œë¡œ í™•ì¥) [[0] [1] [2] [3] [4] ...]\n",
    "    # np.arange(d_model)[None, :]: ì„ë² ë”© ì°¨ì› ì¸ë±ìŠ¤ (ê°€ë¡œë¡œ í™•ì¥) [[0 1 2 3 4 ...]]\n",
    "    # ê²°ê³¼ì ìœ¼ë¡œ (position, d_model) í¬ê¸°ì˜ ê°ë„ í–‰ë ¬(angle_rads)ì„ ìƒì„±\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    # ì§ìˆ˜ ì¸ë±ìŠ¤(0, 2, 4...) â†’ ì‚¬ì¸(Sin) ì ìš©\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # í™€ìˆ˜ ì¸ë±ìŠ¤(1, 3, 5...) â†’ ì½”ì‚¬ì¸(Cos) ì ìš©\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # Positional Encodingì„ 3ì°¨ì›ìœ¼ë¡œ í™•ì¥ (Batch ì°¨ì› ì¶”ê°€)\n",
    "    # (1, position, d_model) í˜•íƒœ â†’ ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•´ ì—¬ëŸ¬ ë¬¸ì¥ì— ì ìš© ê°€ëŠ¥\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "\n",
    "max_len = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [MY CODE] TransformerLayer - Layer normalization, dropout, residual connection êµ¬í˜„"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, dff, n_heads, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.d_model = d_model\n",
    "    self.dff = dff\n",
    "\n",
    "    # ğŸ”¹ ê¸°ì¡´ Self-Attentionì„ MultiHeadAttentionìœ¼ë¡œ ë³€ê²½\n",
    "    self.mha = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "\n",
    "    # ğŸ”¹ Layer Normalization ì¶”ê°€\n",
    "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    # ğŸ”¹ Dropout ì¶”ê°€\n",
    "    self.dropout1 = nn.Dropout(dropout_rate)\n",
    "    self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    # ğŸ”¹ Feed Forward Network (FFN)\n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(d_model, dff),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dff, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    # ğŸ”¹ Multi-Head Attention + Dropout + Residual Connection + LayerNorm\n",
    "    x1 = self.mha(x, mask)\n",
    "    x1 = self.dropout1(x1)\n",
    "    x1 = self.layer_norm1(x1 + x)\n",
    "\n",
    "    # ğŸ”¹ Feed Forward Network + Dropout + Residual Connection + LayerNorm\n",
    "    x2 = self.ffn(x1)\n",
    "    x2 = self.dropout2(x2)\n",
    "    x2 = self.layer_norm2(x2 + x1)\n",
    "\n",
    "    return x2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8MaiCGh8TsDH"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, n_layers, dff, n_heads):\n",
    "    super().__init__()\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.d_model = d_model\n",
    "    self.n_layers = n_layers\n",
    "    self.dff = dff\n",
    "\n",
    "    # ğŸ”¹ ì„ë² ë”© ì¸µ: ë‹¨ì–´ì˜ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ d_model ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    # ğŸ”¹ í¬ì§€ì…”ë„ ì¸ì½”ë”©: ì…ë ¥ í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë¶€ì—¬í•˜ëŠ” ì—­í• \n",
    "    # ìœ„ì¹˜ì— ë”°ë¼ ë‹¤ë¥¸ ì‚¬ì¸(sin) ë° ì½”ì‚¬ì¸(cos) ê°’ì„ ë”í•´ ìˆœì„œë¥¼ ì¸ì‹ ê°€ëŠ¥\n",
    "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
    "\n",
    "    # ğŸ”¹ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    # n_layers ê°œìˆ˜ë§Œí¼ TransformerLayerë¥¼ ìŒ“ìŒ\n",
    "    # ê° ë ˆì´ì–´ëŠ” ë©€í‹°í—¤ë“œ ì–´í…ì…˜(MHA) + FFNìœ¼ë¡œ êµ¬ì„±\n",
    "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff, n_heads) for _ in range(n_layers)])\n",
    "\n",
    "    # ğŸ”¹ ìµœì¢… ì„ í˜• ë¶„ë¥˜ê¸° (1ì°¨ì› ì¶œë ¥)\n",
    "    # ë¬¸ì¥ì˜ ì²« ë²ˆì§¸ í† í°(ì£¼ë¡œ [CLS] í† í°)ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰\n",
    "    self.classification = nn.Linear(d_model, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # ğŸ”¹ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬\n",
    "    mask = (x == tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    #mask = mask[:, None, :]\n",
    "    # mask shape: (B, 1, 1, S)\n",
    "    # ì–´í…ì…˜ ìŠ¤ì½”ì–´ì—ì„œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ë˜ì–´ (B, H, S, S)ë¡œ í™•ì¥ë¨\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    # ğŸ”¹ ì„ë² ë”© ë° í¬ì§€ì…”ë„ ì¸ì½”ë”© ì ìš©\n",
    "    x = self.embedding(x)\n",
    "    x = x * sqrt(self.d_model)\n",
    "    x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "    # ğŸ”¹ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ í†µê³¼\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, mask)\n",
    "\n",
    "    # ğŸ”¹ ì²« ë²ˆì§¸ í† í°([CLS])ì„ ë¶„ë¥˜ê¸°ë¡œ ì „ë‹¬\n",
    "    x = x[:, 0]\n",
    "    x = self.classification(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    preds = model(inputs)\n",
    "    # preds = torch.argmax(preds, dim=-1)\n",
    "    preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def plot_acc(train_accuracies, test_accuracies, label1='train', label2='test'):\n",
    "  x = np.arange(len(train_accuracies))\n",
    "\n",
    "  plt.plot(x, train_accuracies, label=label1)\n",
    "  plt.plot(x, test_accuracies, label=label2)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDq05OlAb2lB"
   },
   "source": [
    "## [MY CODE] í•™ìŠµ\n",
    "- 5-layer 4-head\n",
    "- ì €ì¥/ë¡œë“œ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YHVVsWBPQmnv",
    "outputId": "64b5790f-7649-4a47-95f8-bebe158aba4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒˆë¡­ê²Œ ì‹œì‘~\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "lr = 0.001\n",
    "n_layers = 5\n",
    "n_heads  = 4\n",
    "\n",
    "model = TextClassifier(len(tokenizer), 32, n_layers, 32, n_heads)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 50\n",
    "start_epoch = 0\n",
    "\n",
    "time_list = []\n",
    "average_loss_list = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "checkpoint_path = 'checkpoint_TextClassifier.pth'\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']  # ì´ì–´ì„œ ì‹œì‘í•  ì—í¬í¬\n",
    "    time_list = checkpoint['time_list']\n",
    "    average_loss_list = checkpoint['average_loss_list']\n",
    "    train_accuracies = checkpoint['train_accuracies']\n",
    "    test_accuracies = checkpoint['test_accuracies']\n",
    "    for epoch in range(0, start_epoch):\n",
    "        print(f\"Epoch {epoch+1:3d} |\"\n",
    "        f\" Time: {time_list[epoch]:.2f} seconds |\"\n",
    "        f\" Loss: {average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Train Acc: {train_accuracies[epoch]:.3f} |\"\n",
    "        f\" Test Acc: {test_accuracies[epoch]:.3f}\")\n",
    "\n",
    "    if start_epoch < n_epochs -1:\n",
    "        print(f\"ì´ì–´ì„œ ì‹œì‘~ {start_epoch + 1}.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ìƒˆë¡­ê²Œ ì‹œì‘~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTextClassifier                           [64, 1]                   12,800\nâ”œâ”€Embedding: 1-1                         [64, 400, 32]             976,704\nâ”œâ”€ModuleList: 1-2                        --                        --\nâ”‚    â””â”€TransformerLayer: 2-1             [64, 400, 32]             --\nâ”‚    â”‚    â””â”€MultiHeadAttention: 3-1      [64, 400, 32]             4,224\nâ”‚    â”‚    â””â”€Dropout: 3-2                 [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-3               [64, 400, 32]             64\nâ”‚    â”‚    â””â”€Sequential: 3-4              [64, 400, 32]             2,112\nâ”‚    â”‚    â””â”€Dropout: 3-5                 [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-6               [64, 400, 32]             64\nâ”‚    â””â”€TransformerLayer: 2-2             [64, 400, 32]             --\nâ”‚    â”‚    â””â”€MultiHeadAttention: 3-7      [64, 400, 32]             4,224\nâ”‚    â”‚    â””â”€Dropout: 3-8                 [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-9               [64, 400, 32]             64\nâ”‚    â”‚    â””â”€Sequential: 3-10             [64, 400, 32]             2,112\nâ”‚    â”‚    â””â”€Dropout: 3-11                [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-12              [64, 400, 32]             64\nâ”‚    â””â”€TransformerLayer: 2-3             [64, 400, 32]             --\nâ”‚    â”‚    â””â”€MultiHeadAttention: 3-13     [64, 400, 32]             4,224\nâ”‚    â”‚    â””â”€Dropout: 3-14                [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-15              [64, 400, 32]             64\nâ”‚    â”‚    â””â”€Sequential: 3-16             [64, 400, 32]             2,112\nâ”‚    â”‚    â””â”€Dropout: 3-17                [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-18              [64, 400, 32]             64\nâ”‚    â””â”€TransformerLayer: 2-4             [64, 400, 32]             --\nâ”‚    â”‚    â””â”€MultiHeadAttention: 3-19     [64, 400, 32]             4,224\nâ”‚    â”‚    â””â”€Dropout: 3-20                [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-21              [64, 400, 32]             64\nâ”‚    â”‚    â””â”€Sequential: 3-22             [64, 400, 32]             2,112\nâ”‚    â”‚    â””â”€Dropout: 3-23                [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-24              [64, 400, 32]             64\nâ”‚    â””â”€TransformerLayer: 2-5             [64, 400, 32]             --\nâ”‚    â”‚    â””â”€MultiHeadAttention: 3-25     [64, 400, 32]             4,224\nâ”‚    â”‚    â””â”€Dropout: 3-26                [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-27              [64, 400, 32]             64\nâ”‚    â”‚    â””â”€Sequential: 3-28             [64, 400, 32]             2,112\nâ”‚    â”‚    â””â”€Dropout: 3-29                [64, 400, 32]             --\nâ”‚    â”‚    â””â”€LayerNorm: 3-30              [64, 400, 32]             64\nâ”œâ”€Linear: 1-3                            [64, 1]                   33\n==========================================================================================\nTotal params: 1,021,857\nTrainable params: 1,009,057\nNon-trainable params: 12,800\nTotal mult-adds (Units.MEGABYTES): 64.58\n==========================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 268.70\nParams size (MB): 4.04\nEstimated Total Size (MB): 272.94\n=========================================================================================="
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "summary(model, input_size=(64, max_len), dtypes=[torch.int64])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 408929,
     "status": "ok",
     "timestamp": 1723896769492,
     "user": {
      "displayName": "ì¡°ìŠ¹í˜",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "al_b56TYRILq",
    "outputId": "90a56264-4ef3-4def-e7b7-df4b5cd3c305"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m      8\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m---> 10\u001B[0m preds \u001B[38;5;241m=\u001B[39m model(inputs)[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     11\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(preds, labels)\n\u001B[1;32m     12\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[30], line 35\u001B[0m, in \u001B[0;36mTextClassifier.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     32\u001B[0m seq_len \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# ğŸ”¹ ì„ë² ë”© ë° í¬ì§€ì…”ë„ ì¸ì½”ë”© ì ìš©\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(x)\n\u001B[1;32m     36\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m*\u001B[39m sqrt(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model)\n\u001B[1;32m     37\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_encoding[:, :seq_len]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39membedding(\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    192\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[1;32m    193\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_idx,\n\u001B[1;32m    194\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_norm,\n\u001B[1;32m    195\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm_type,\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_grad_by_freq,\n\u001B[1;32m    197\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparse,\n\u001B[1;32m    198\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2545\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2547\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2548\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2549\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2550\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2551\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39membedding(weight, \u001B[38;5;28minput\u001B[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "model.to(device)  # ë‹¤ì‹œ MPSë¡œ ì´ë™ (ì¤‘ìš”!)\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "  start_time = time.time()  # ì—í¬í¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for data in train_loader:\n",
    "    model.zero_grad()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "    preds = model(inputs)[..., 0]\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time  # ì—í¬í¬ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°\n",
    "\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    time_list.append(epoch_time)\n",
    "    average_loss_list.append(average_loss)\n",
    "\n",
    "    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'time_list': time_list,\n",
    "        'average_loss_list': average_loss_list,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:3d} |\"\n",
    "    f\" Time: {epoch_time:.2f} seconds |\"\n",
    "    f\" Loss: {average_loss:.2f} |\"\n",
    "    f\" Train Acc: {train_acc:.3f} |\"\n",
    "    f\" Test Acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_acc(train_accuracies, test_accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
