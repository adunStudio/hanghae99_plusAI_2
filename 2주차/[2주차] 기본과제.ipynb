{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [MY CODE] Last word prediction dataset 준비"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "HOdhoBVA1zcu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimhongil/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "  max_len= 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    # 라벨을 감정이 아닌 각 텍스트의 뒷자리 단어(토큰) 2개로 변경\n",
    "    tokenizer_output = tokenizer(row['text'], truncation=True, max_length=max_len)\n",
    "    labels.append(tokenizer_output.input_ids[-2])\n",
    "    texts.append(torch.LongTensor(tokenizer_output.input_ids[:-2]))\n",
    "\n",
    "  # 패딩 처리(길이가 맞지 않는 경우를 위해)\n",
    "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#[MY CODE] Shape와 입력/출력 확인(빈도 등)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n 'label': 0}"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텍스트 (input_ids) shape: torch.Size([64, 398])\n",
      "출력 라벨 (labels) shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, labels = batch  # DataLoader에서 배치 추출\n",
    "    print(f\"입력 텍스트 (input_ids) shape: {inputs.shape}\")\n",
    "    print(f\"출력 라벨 (labels) shape: {labels.shape}\")\n",
    "    break  # 첫 번째 배치만 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텍스트 (문장): PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "출력 라벨 (토큰): ['.']\n",
      "출력 라벨 (문장): .\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, labels = batch  # DataLoader에서 배치 추출\n",
    "\n",
    "    # 첫 번째 샘플만 확인\n",
    "    idx = 0\n",
    "    first_input = inputs[idx]\n",
    "    first_label = labels[idx]\n",
    "\n",
    "    # 입력 텍스트 디코딩 (토큰 단위)\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(first_input)\n",
    "    decoded_text = tokenizer.decode(first_input)\n",
    "\n",
    "    # 라벨 디코딩 (정답 토큰 ID를 텍스트로 변환)\n",
    "    label_token = tokenizer.convert_ids_to_tokens([first_label])  # 라벨은 단일 토큰\n",
    "    label_text = tokenizer.decode([first_label])  # 라벨을 문장으로 변환\n",
    "\n",
    "    # 결과 출력\n",
    "    #print(\"입력 텍스트 (토큰):\", decoded_tokens)\n",
    "    print(\"입력 텍스트 (문장):\", decoded_text[-100:])\n",
    "    print(\"출력 라벨 (토큰):\", label_token)\n",
    "    print(\"출력 라벨 (문장):\", label_text)\n",
    "\n",
    "    break  # 첫 번째 배치만 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니크 라벨 목록: {8195, 8201, 8212, 8226, 24623, 16437, 8246, 8257, 16451, 8266, 16465, 16467, 24665, 8282, 16481, 8290, 8292, 8307, 24703, 8321, 24714, 24759, 8378, 16571, 8385, 16596, 24795, 24808, 8428, 8429, 8434, 8440, 16636, 8450, 16643, 8472, 8484, 8485, 24871, 16681, 8490, 8489, 16701, 8516, 16709, 8518, 8527, 8529, 8562, 16755, 24947, 8568, 8569, 8572, 24965, 8586, 8589, 8596, 8605, 8607, 8631, 8632, 25020, 16833, 8674, 25066, 16880, 25093, 8713, 16914, 8739, 8740, 8754, 16948, 8785, 8794, 8795, 8797, 8814, 25219, 8836, 8847, 25248, 17075, 8884, 8889, 17083, 8909, 25314, 25325, 8943, 8948, 8954, 17153, 17156, 8973, 9000, 9004, 9009, 9019, 17211, 9027, 25423, 9056, 9061, 17274, 25469, 9092, 17289, 9102, 17298, 9110, 9117, 9120, 17312, 9122, 25508, 9129, 17324, 9143, 9145, 17339, 9148, 25539, 17363, 9179, 999, 1000, 1001, 1003, 1004, 1005, 1006, 1007, 9200, 1008, 9202, 1010, 1012, 1011, 1009, 1015, 1013, 1014, 9210, 1019, 1017, 1016, 1018, 1020, 25600, 1024, 1026, 1025, 1028, 1029, 9219, 1023, 25608, 1033, 1027, 1035, 1036, 1037, 1038, 25614, 1040, 1039, 1042, 1041, 1043, 1045, 1046, 1047, 1049, 1050, 1051, 1052, 9243, 1053, 1055, 1056, 1054, 1060, 1061, 1062, 1065, 1066, 9260, 17453, 1074, 25653, 17463, 17472, 1089, 1092, 9288, 9289, 9313, 9364, 9365, 25749, 9372, 9388, 9401, 9411, 9414, 9415, 9416, 9422, 9449, 9458, 9461, 9467, 9476, 9479, 9481, 25869, 9496, 9508, 17704, 9515, 17710, 9527, 9530, 9535, 17743, 9555, 25953, 9574, 9575, 9577, 9578, 9579, 25969, 9597, 9599, 9609, 9616, 9617, 17831, 9643, 9647, 9649, 9658, 17868, 9678, 9706, 17904, 9714, 9727, 17953, 9762, 9769, 9777, 17975, 17979, 9811, 18006, 18012, 18020, 18025, 18029, 18031, 9846, 9855, 18058, 9866, 18062, 18075, 26279, 9899, 9901, 26285, 9916, 18113, 26316, 9951, 26373, 18182, 9991, 26375, 9996, 10015, 18207, 10018, 10021, 10023, 18217, 18224, 10036, 18230, 10047, 18261, 10071, 26471, 18293, 10109, 10115, 10129, 10136, 10140, 18336, 10184, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 18403, 2019, 26597, 2020, 2021, 2022, 2023, 2024, 2025, 18410, 2027, 2026, 2028, 2032, 2030, 2031, 2029, 2036, 2033, 2038, 2039, 2035, 2034, 10231, 2040, 2044, 2041, 2042, 2043, 2045, 2049, 2050, 2051, 18434, 2053, 2052, 2055, 2054, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 18458, 2076, 2077, 2075, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 10270, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 10278, 2096, 2097, 2098, 2037, 2100, 2101, 2102, 2046, 2104, 2099, 2106, 2107, 2047, 2108, 2109, 2111, 2112, 2113, 2114, 2115, 2048, 2117, 2116, 2119, 2121, 2122, 2123, 18507, 2125, 2126, 2124, 2128, 2129, 2130, 2131, 2132, 2127, 2134, 2135, 2137, 2138, 2139, 2140, 10334, 2143, 2144, 2145, 2146, 2147, 2149, 2151, 2152, 2153, 2056, 2154, 2156, 2157, 2158, 2159, 2160, 2155, 2161, 2163, 2166, 26743, 2168, 2169, 2170, 2171, 10363, 2173, 2172, 2175, 2174, 2176, 2178, 2179, 2180, 10371, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 18577, 10205, 2196, 2197, 2198, 2199, 2200, 18584, 2202, 2203, 2204, 2205, 10398, 26775, 2207, 2206, 2210, 2213, 2214, 2215, 2216, 2218, 2219, 2220, 2222, 2224, 10418, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 10424, 2235, 2236, 2237, 2238, 2239, 10431, 2240, 2242, 2243, 18627, 2245, 2246, 10439, 26824, 2253, 2256, 2261, 2263, 2078, 2265, 10456, 2268, 2269, 2271, 2272, 2273, 26851, 10468, 2275, 2279, 2282, 2283, 2287, 2288, 10224, 2290, 2289, 2292, 2293, 2294, 10487, 2296, 2295, 2298, 2302, 2303, 2304, 2305, 2086, 2307, 26884, 2309, 2310, 10503, 2312, 18697, 2314, 2315, 18691, 18698, 10510, 2319, 10514, 2323, 2324, 2327, 18712, 2329, 2330, 10520, 10523, 18718, 2335, 2336, 2338, 2339, 2342, 2345, 2348, 2349, 2350, 2354, 26931, 26932, 2356, 2359, 2360, 2361, 2362, 10554, 2364, 2363, 10556, 10559, 2368, 2369, 2367, 2371, 18748, 2373, 2375, 2376, 2377, 2378, 2379, 2381, 10576, 2385, 2386, 2384, 2388, 2389, 2103, 2391, 2393, 2394, 10587, 10586, 2395, 2399, 2400, 2401, 2402, 2404, 10597, 26985, 2409, 2412, 2413, 2414, 26989, 2417, 10610, 2419, 2421, 2422, 2424, 2425, 2426, 2110, 2428, 10191, 2431, 2434, 2435, 2437, 2438, 10631, 2439, 18824, 2442, 2443, 2444, 2445, 2446, 18823, 2450, 10643, 2453, 2455, 10258, 2460, 10654, 18847, 2464, 10655, 2466, 2467, 2468, 2469, 2471, 2472, 2475, 2476, 2477, 2480, 2481, 2482, 2483, 2487, 2488, 27063, 18874, 2491, 2492, 2497, 2498, 2500, 10693, 2502, 18886, 2504, 2505, 2503, 2507, 2514, 2515, 2516, 10708, 2518, 2517, 27096, 2521, 2522, 2520, 10271, 2529, 10272, 2531, 2535, 2540, 27118, 2542, 2543, 2545, 2546, 2544, 2552, 2553, 2558, 2559, 2560, 2562, 2564, 2566, 2567, 2568, 2569, 18954, 2571, 2572, 27148, 2576, 2577, 2579, 2580, 2581, 2582, 2583, 27159, 2585, 2587, 2589, 2592, 2593, 2594, 2595, 2598, 2599, 2601, 2605, 2606, 2607, 10800, 2610, 2611, 2612, 2613, 2614, 2615, 2617, 2618, 2619, 10812, 2620, 2622, 2625, 10819, 2630, 2632, 2633, 27214, 27217, 10833, 2643, 2642, 2645, 10840, 2648, 10841, 2651, 10844, 2653, 10843, 2655, 10848, 2657, 2659, 2662, 2664, 10299, 2666, 2667, 2668, 2669, 2672, 2673, 2674, 2677, 2678, 2679, 2681, 10874, 2682, 19067, 2686, 10880, 2689, 2690, 2694, 2696, 10889, 2698, 2697, 19084, 2699, 2702, 2709, 2710, 2711, 18449, 2713, 2714, 2716, 2717, 2720, 2721, 2723, 2724, 2725, 2726, 10919, 2728, 2729, 19118, 2735, 2734, 2738, 10932, 19124, 2743, 19128, 2745, 2746, 2749, 2754, 2755, 2757, 19142, 2758, 2759, 2763, 2764, 2765, 19148, 2767, 10958, 2769, 2771, 2774, 2775, 2779, 2781, 2784, 2785, 19170, 2790, 2791, 2792, 2793, 19181, 2797, 19182, 2800, 2802, 2804, 2806, 2808, 2812, 2814, 11007, 2818, 11012, 2821, 2824, 2830, 2833, 2836, 2838, 2839, 2841, 2842, 2843, 2844, 11038, 2848, 2850, 2851, 2852, 27427, 2854, 2855, 19240, 2856, 2860, 2866, 2869, 11065, 11067, 2875, 2877, 2878, 2879, 2876, 2890, 11082, 11084, 2894, 2895, 2902, 2903, 2908, 2912, 2915, 19301, 2919, 2920, 11113, 2923, 2926, 2927, 2928, 19313, 2931, 2935, 2937, 2941, 2946, 11139, 2953, 19337, 2961, 11162, 27547, 2972, 11167, 2978, 11171, 2980, 2984, 2987, 2989, 2995, 2997, 3000, 19384, 3003, 3004, 3011, 3012, 3013, 19399, 3022, 3025, 3031, 3033, 19424, 3041, 3042, 11235, 3044, 3045, 3046, 3047, 3048, 3051, 3053, 3054, 11247, 3056, 3060, 3064, 3065, 11263, 3071, 11265, 11269, 3080, 3081, 19466, 3084, 3085, 3087, 3088, 3089, 3090, 3091, 3092, 19477, 3096, 3100, 3102, 3103, 19486, 3105, 3109, 3110, 3111, 3113, 27693, 3117, 3120, 27699, 3124, 3126, 27707, 10392, 3134, 3135, 11333, 3143, 27720, 3145, 11338, 27723, 3149, 11342, 11341, 3152, 3153, 3151, 3159, 3163, 3166, 3167, 3168, 11361, 3170, 3174, 3178, 11374, 19566, 27760, 3185, 3182, 3193, 27774, 11390, 3198, 3207, 3210, 3213, 3216, 3220, 3224, 3227, 3228, 3233, 3235, 3236, 3238, 3241, 3243, 3246, 3248, 3252, 3257, 3259, 11452, 11453, 3267, 3270, 11463, 3275, 3276, 11471, 27858, 3283, 11475, 3285, 3286, 3287, 3288, 3291, 3295, 3297, 3305, 11498, 3308, 3310, 3312, 27889, 3315, 11512, 11514, 11516, 3325, 11519, 3328, 27903, 3331, 3332, 11529, 11531, 3340, 3345, 3347, 3350, 3351, 3352, 19739, 3357, 3363, 3364, 3366, 3367, 3371, 3374, 3376, 3382, 3383, 11575, 3385, 3389, 3391, 3392, 3395, 3396, 3401, 3403, 3406, 3407, 3408, 27987, 2306, 3413, 3419, 3422, 27999, 3426, 3427, 3432, 3433, 3435, 3436, 3437, 3439, 3442, 28019, 19828, 3446, 3456, 3459, 11654, 19846, 3462, 3466, 3468, 11663, 3475, 11669, 3480, 3484, 28061, 11680, 3490, 3492, 3494, 3496, 3497, 3501, 3504, 3505, 11703, 3512, 11707, 3516, 3517, 3521, 3522, 3523, 3526, 3531, 3535, 3538, 11731, 3544, 11737, 3545, 11741, 28128, 3554, 3556, 3560, 11757, 28147, 3573, 3574, 19957, 11771, 3581, 19968, 3586, 3593, 3594, 11788, 3597, 28174, 3599, 19986, 3603, 3602, 3606, 3609, 11802, 3611, 19996, 3630, 3632, 3637, 3643, 3644, 3649, 3651, 3656, 3657, 3660, 3663, 3666, 20051, 3669, 3671, 3672, 3674, 11867, 3677, 3683, 3686, 3689, 3690, 3692, 3698, 3702, 11898, 3710, 20100, 3723, 3727, 3728, 3733, 3737, 3748, 3752, 3754, 3757, 11951, 3762, 3763, 3764, 28341, 3768, 3770, 3772, 3773, 3782, 3786, 3790, 3796, 3800, 20187, 3807, 3809, 3811, 12005, 3815, 12015, 3824, 3825, 3827, 3835, 12027, 3836, 12031, 3841, 28419, 3848, 3849, 3854, 12054, 12055, 12056, 28450, 3881, 12074, 3883, 3885, 12081, 3893, 3894, 3896, 3898, 3903, 3908, 3909, 3915, 3923, 28500, 3929, 3931, 3938, 3942, 3944, 3946, 3947, 10555, 3954, 12147, 20341, 20342, 3960, 20345, 28541, 3968, 20355, 3971, 12167, 3976, 3979, 3980, 3984, 3988, 12181, 20376, 3993, 3995, 3996, 3997, 12193, 4011, 4012, 4016, 4017, 4025, 4027, 4028, 4030, 12225, 4038, 28616, 4040, 4044, 12237, 4049, 4066, 4070, 4071, 4072, 12268, 4076, 4079, 4083, 4088, 4089, 4092, 20478, 4095, 12289, 4099, 4102, 4103, 28681, 4110, 4116, 4117, 4121, 4122, 4126, 4127, 4136, 4142, 4143, 20531, 4147, 4150, 12344, 4152, 4153, 12347, 4164, 4165, 4166, 28741, 12362, 4178, 4180, 4181, 4189, 12382, 4198, 4203, 4204, 4205, 4208, 4212, 4217, 4221, 4233, 28810, 12429, 12436, 4246, 4248, 4257, 20649, 4270, 4276, 4277, 12472, 4282, 4283, 12476, 4286, 20672, 4289, 20673, 20676, 4297, 4298, 4299, 4300, 4305, 4310, 4312, 4323, 4326, 12520, 4332, 4333, 4338, 12536, 12537, 4353, 4356, 4364, 4367, 28945, 28946, 4375, 4378, 4379, 4380, 4381, 4382, 4383, 4388, 12580, 4391, 12584, 4395, 20782, 4401, 4402, 28979, 4405, 4411, 4414, 20801, 12611, 4424, 4435, 4438, 20824, 4441, 12635, 4450, 4454, 4459, 20846, 4463, 4468, 4477, 4486, 4487, 4490, 4492, 4496, 12689, 12688, 4506, 12700, 4509, 4510, 12703, 4515, 4516, 4517, 29094, 4522, 12721, 4531, 4540, 4542, 29122, 4547, 12743, 20945, 4566, 4569, 12767, 4576, 20961, 12768, 4586, 4590, 12785, 4599, 4602, 4608, 12803, 4616, 21001, 29196, 4627, 4632, 4634, 4637, 4638, 4647, 4648, 4654, 12849, 4658, 4660, 4669, 4670, 21055, 21054, 4675, 4676, 4679, 12872, 4687, 4690, 4691, 4692, 4695, 4708, 4710, 4717, 4728, 4740, 4754, 4756, 4757, 4760, 29337, 21146, 4763, 29348, 4773, 4776, 29353, 4781, 4788, 4789, 29370, 4797, 12990, 4799, 4808, 4817, 4819, 21209, 4825, 4832, 21222, 13031, 4845, 29421, 4848, 4850, 13044, 21246, 4863, 21250, 4869, 4872, 4873, 13068, 4895, 4899, 13093, 4904, 4906, 21294, 13109, 4918, 4920, 21307, 4923, 4930, 29515, 4940, 13133, 4942, 21323, 4945, 4953, 4954, 4957, 4962, 4965, 4966, 4968, 13162, 29552, 13172, 4983, 13186, 4995, 4997, 21382, 29573, 4998, 5003, 5005, 5008, 5009, 5015, 5017, 5019, 13213, 5024, 5027, 13223, 5032, 13231, 5041, 5043, 5048, 5051, 5053, 21438, 5054, 13250, 5060, 5064, 5070, 21459, 21466, 5084, 5086, 5092, 13288, 5099, 5104, 5107, 5110, 5121, 5125, 5130, 5136, 1021, 1022, 5156, 5157, 13357, 5166, 5168, 13363, 13366, 5177, 21562, 5186, 21570, 5189, 5192, 5197, 5199, 13398, 5215, 5216, 5223, 5232, 5241, 5249, 5253, 5256, 5257, 5261, 5263, 13459, 5268, 21669, 5292, 5293, 21681, 5297, 21682, 21686, 5305, 13498, 5308, 5312, 5313, 5321, 5328, 13534, 5343, 13536, 5349, 21735, 5354, 21744, 5363, 5365, 5366, 21751, 5369, 5372, 21766, 5390, 5391, 5392, 5394, 5397, 5399, 5403, 13597, 5405, 5407, 21791, 5409, 5426, 13623, 5433, 5436, 5443, 5448, 5457, 5466, 5468, 5469, 13662, 5470, 5476, 13669, 21864, 5481, 13675, 13676, 5493, 21883, 5501, 5504, 21890, 13704, 5514, 21907, 5524, 5525, 13718, 5530, 5537, 21933, 21934, 5553, 5562, 5570, 5572, 13764, 21961, 13769, 5585, 5587, 5593, 5599, 5603, 13798, 5607, 5609, 13805, 5613, 5627, 5630, 13822, 22017, 5637, 5639, 5650, 5651, 13844, 22038, 13851, 5662, 22047, 5665, 22050, 5667, 5671, 5675, 5683, 5684, 22071, 13880, 5691, 5694, 5696, 17416, 22092, 13911, 5722, 5729, 5735, 5736, 22123, 5747, 22133, 13950, 5773, 13970, 13972, 5783, 5790, 5791, 5794, 5796, 5799, 5802, 5805, 14003, 5830, 22220, 14030, 5841, 14036, 5844, 5845, 14046, 5855, 22248, 22249, 5866, 5867, 5869, 5870, 5875, 25593, 5888, 5889, 14081, 5896, 14092, 5912, 5914, 5922, 5927, 5931, 5933, 5942, 5948, 5949, 14141, 14145, 5953, 5959, 5965, 14163, 5978, 5987, 5997, 14189, 6013, 6018, 6020, 6034, 6038, 6040, 6047, 6051, 6057, 6058, 14255, 14263, 6080, 14277, 14285, 22478, 6097, 6100, 14296, 6118, 6122, 6128, 6132, 6135, 6149, 14341, 6170, 6174, 6178, 6199, 14392, 22599, 6216, 14414, 14415, 22610, 6240, 14434, 6251, 6256, 22662, 6279, 6292, 6293, 6294, 14486, 6298, 6302, 6314, 6316, 6321, 6326, 6335, 6339, 6357, 6359, 6361, 6370, 6371, 14563, 6373, 6379, 6384, 6385, 22773, 6396, 6398, 6404, 6405, 14598, 14601, 6414, 22807, 6424, 6433, 22819, 14633, 6443, 6453, 14652, 6460, 6469, 6473, 6477, 6480, 14672, 6491, 6494, 6510, 6513, 14706, 22901, 6517, 14711, 6524, 14718, 6542, 14742, 6553, 6558, 6560, 22957, 6573, 6581, 6582, 6583, 6593, 14798, 6625, 14818, 14832, 6651, 6653, 6654, 6659, 6662, 14856, 6669, 6670, 14862, 6688, 6693, 14891, 6703, 6707, 6719, 6720, 14913, 6732, 14928, 6739, 6741, 6749, 6755, 6773, 23168, 6786, 6791, 14984, 6806, 6810, 15003, 6812, 6824, 6826, 6827, 6847, 6848, 23238, 6857, 6881, 6892, 6894, 6896, 23283, 6904, 6907, 23292, 6914, 6927, 6945, 15140, 6958, 23347, 6965, 6968, 6969, 23358, 6987, 6991, 15185, 7001, 15194, 7009, 15202, 23408, 7028, 7034, 15237, 7046, 23441, 23459, 7078, 7079, 7087, 23487, 7106, 7107, 15308, 7118, 7121, 23514, 7162, 7164, 23551, 7171, 7178, 7182, 7201, 7209, 15401, 7213, 15417, 15418, 23626, 7244, 7256, 15450, 7262, 7265, 23653, 23654, 23667, 7284, 15480, 7309, 7329, 7332, 15539, 7354, 15549, 7370, 15580, 7389, 7393, 7405, 15608, 7420, 7432, 7436, 7437, 15640, 15653, 23847, 7467, 7481, 23873, 15683, 7494, 7499, 15703, 15716, 15723, 15725, 23929, 7551, 15743, 23943, 15752, 7585, 15789, 7603, 7627, 7630, 7632, 7635, 7648, 15854, 15859, 24053, 7678, 15876, 7685, 15883, 15886, 7697, 7699, 24084, 7723, 15916, 15921, 24119, 15935, 7743, 7756, 15966, 15978, 7789, 7800, 24185, 7806, 7807, 7815, 24216, 7834, 16033, 7842, 16036, 7849, 16042, 7857, 24246, 7874, 7880, 7886, 24282, 7903, 7908, 7929, 16126, 7935, 16129, 7959, 7961, 16157, 7966, 7968, 7971, 7977, 7980, 7982, 7987, 7994, 8009, 16215, 24407, 16219, 8038, 8040, 8042, 16234, 16236, 16251, 8066, 8067, 16258, 8072, 24457, 24471, 8091, 8095, 8099, 8101, 8106, 24493, 8113, 24497, 8126, 8128, 24514, 16324, 8134, 24525, 24529, 8146, 16351, 16360, 8178, 16371, 16372, 24568, 8190}\n",
      "총 유니크 라벨 개수: 2253\n",
      "라벨: . (토큰 ID: 1012) - 등장 횟수: 13945\n",
      "라벨: ! (토큰 ID: 999) - 등장 횟수: 2159\n",
      "라벨: 10 (토큰 ID: 2184) - 등장 횟수: 517\n",
      "라벨: ) (토큰 ID: 1007) - 등장 횟수: 496\n",
      "라벨: > (토큰 ID: 1028) - 등장 횟수: 438\n",
      "라벨: ? (토큰 ID: 1029) - 등장 횟수: 380\n",
      "라벨: the (토큰 ID: 1996) - 등장 횟수: 261\n",
      "라벨: \" (토큰 ID: 1000) - 등장 횟수: 252\n",
      "라벨: , (토큰 ID: 1010) - 등장 횟수: 208\n",
      "라벨: * (토큰 ID: 1008) - 등장 횟수: 190\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_labels = []  # 모든 라벨을 저장할 리스트\n",
    "label_counts = Counter(all_labels)\n",
    "decoded_labels = tokenizer.convert_ids_to_tokens(list(label_counts.keys()))\n",
    "\n",
    "# DataLoader에서 배치 순회\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch  # 라벨 추출\n",
    "    all_labels.extend(labels.tolist())  # 리스트로 변환 후 확장\n",
    "\n",
    "# 라벨의 유니크 값 확인\n",
    "unique_labels = set(all_labels)\n",
    "print(\"유니크 라벨 목록:\", unique_labels)\n",
    "print(f\"총 유니크 라벨 개수: {len(unique_labels)}\")\n",
    "\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "top_10_labels = label_counts.most_common(10)  # 상위 10개 라벨\n",
    "decoded_top_10 = tokenizer.convert_ids_to_tokens([item[0] for item in top_10_labels])\n",
    "\n",
    "for word, (token_id, count) in zip(decoded_top_10, top_10_labels):\n",
    "    print(f\"라벨: {word} (토큰 ID: {token_id}) - 등장 횟수: {count}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-FshZcTZBQ2"
   },
   "source": [
    "## Self-attention\n",
    "\n",
    "이번에는 self-attention을 구현해보겠습니다.\n",
    "Self-attention은 shape이 (B, S, D)인 embedding이 들어왔을 때 attention을 적용하여 새로운 representation을 만들어내는 module입니다.\n",
    "여기서 B는 batch size, S는 sequence length, D는 embedding 차원입니다.\n",
    "구현은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "MBlMVMZcRAxv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, input_dim, d_model):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.d_model = d_model\n",
    "\n",
    "    self.wq = nn.Linear(input_dim, d_model)\n",
    "    self.wk = nn.Linear(input_dim, d_model)\n",
    "    self.wv = nn.Linear(input_dim, d_model)\n",
    "    self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "    score = score / sqrt(self.d_model)\n",
    "\n",
    "    if mask is not None:\n",
    "      score = score + (mask * -1e9)\n",
    "\n",
    "    score = self.softmax(score)\n",
    "    result = torch.matmul(score, v)\n",
    "    result = self.dense(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S0vMp85ZRNO"
   },
   "source": [
    "대부분은 Transformer 챕터에서 배운 수식들을 그대로 구현한 것에 불과합니다.\n",
    "차이점은 `mask`의 존재여부입니다.\n",
    "이전 챕터에서 우리는 가변적인 text data들에 padding token을 붙여 하나의 matrix로 만든 방법을 배웠습니다.\n",
    "실제 attention 계산에서는 이를 무시해주기 위해 mask를 만들어 제공해주게 됩니다.\n",
    "여기서 mask의 shape은 (B, S, 1)로, 만약 `mask[i, j] = True`이면 그 변수는 padding token에 해당한다는 뜻입니다.\n",
    "이러한 값들을 무시해주는 방법은 shape이 (B, S, S)인 `score`가 있을 때(수업에서 배운 $A$와 동일) `score[i, j]`에 아주 작은 값을 더해주면 됩니다. 아주 작은 값은 예를 들어 `-1000..00 = -1e9` 같은 것이 있습니다.\n",
    "이렇게 작은 값을 더해주고 나면 softmax를 거쳤을 때 0에 가까워지기 때문에 weighted sum 과정에서 padding token에 해당하는 `v` 값들을 무시할 수 있게 됩니다.\n",
    "\n",
    "다음은 self-attention과 feed-forward layer를 구현한 모습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "VZHPCn9AS5Gp"
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, dff):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.d_model = d_model\n",
    "    self.dff = dff\n",
    "\n",
    "    self.sa = SelfAttention(input_dim, d_model)\n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(d_model, dff),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dff, d_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    x = self.sa(x, mask)\n",
    "    x = self.ffn(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_xC9BQJaU4q"
   },
   "source": [
    "보시다시피 self-attention의 구현이 어렵지, Transformer layer 하나 구현하는 것은 수업 때 다룬 그림과 크게 구분되지 않는다는 점을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VYrqTJagS1"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "이번에는 positional encoding을 구현합니다. Positional encoding의 식은 다음과 같습니다:\n",
    "$$\n",
    "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
    "$$\n",
    "\n",
    "이를 Numpy로 구현하여 PyTorch tensor로 변환한 모습은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1723896343031,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Uf_jMQWDUR79",
    "outputId": "534712be-1522-4d32-81b7-87f50a6f1f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "\n",
    "max_len = 400\n",
    "print(positional_encoding(max_len, 256).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5unoDcBva3eN"
   },
   "source": [
    "Positional encoding은 `angle_rads`를 구현하는 과정에서 모두 구현이 되었습니다. 여기서 `angle_rads`의 shape은 (S, D)입니다.\n",
    "우리는 일반적으로 batch로 주어지는 shape이 (B, S, D)인 tensor를 다루기 때문에 마지막에 None을 활용하여 shape을 (1, S, D)로 바꿔주게됩니다.\n",
    "\n",
    "위에서 구현한 `TransformerLayer`와 positional encoding을 모두 합친 모습은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "8MaiCGh8TsDH"
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
    "    super().__init__()\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.d_model = d_model\n",
    "    self.n_layers = n_layers\n",
    "    self.dff = dff\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
    "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
    "\n",
    "    # 이진 분류가 아니므로 토큰 개수로 변경\n",
    "    self.classification = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    mask = (x == tokenizer.pad_token_id)\n",
    "    mask = mask[:, None, :]\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    x = x * sqrt(self.d_model)\n",
    "    x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, mask)\n",
    "\n",
    "    x = x[:, 0]\n",
    "    x = self.classification(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = TextClassifier(len(tokenizer), 32, 2, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXpjPWHjbUK8"
   },
   "source": [
    "기존과 다른 점들은 다음과 같습니다:\n",
    "1. `nn.ModuleList`를 사용하여 여러 layer의 구현을 쉽게 하였습니다.\n",
    "2. Embedding, positional encoding, transformer layer를 거치고 난 후 마지막 label을 예측하기 위해 사용한 값은 `x[:, 0]`입니다. 기존의 RNN에서는 padding token을 제외한 마지막 token에 해당하는 representation을 사용한 것과 다릅니다. 이렇게 사용할 수 있는 이유는 attention 과정을 보시면 첫 번째 token에 대한 representation은 이후의 모든 token의 영향을 받습니다. 즉, 첫 번째 token 또한 전체 문장을 대변하는 의미를 가지고 있다고 할 수 있습니다. 그래서 일반적으로 Transformer를 text 분류에 사용할 때는 이와 같은 방식으로 구현됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDq05OlAb2lB"
   },
   "source": [
    "## 학습\n",
    "\n",
    "학습하는 코드는 기존 실습들과 동일하기 때문에 마지막 결과만 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "YHVVsWBPQmnv",
    "outputId": "64b5790f-7649-4a47-95f8-bebe158aba4f"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "r88BALxO1zc1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    preds = model(inputs)\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "    #preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 408929,
     "status": "ok",
     "timestamp": 1723896769492,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "al_b56TYRILq",
    "outputId": "90a56264-4ef3-4def-e7b7-df4b5cd3c305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Time: 29.61 seconds | Loss: 3.78 | Train Acc: 0.558 | Test Acc: 0.564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[209], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     28\u001B[0m   model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m---> 29\u001B[0m   train_acc \u001B[38;5;241m=\u001B[39m accuracy(model, train_loader)\n\u001B[1;32m     30\u001B[0m   test_acc \u001B[38;5;241m=\u001B[39m accuracy(model, test_loader)\n\u001B[1;32m     31\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m3d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     32\u001B[0m   \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Time: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_time\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     33\u001B[0m   \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maverage_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     34\u001B[0m   \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Train Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     35\u001B[0m   \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Test Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[208], line 9\u001B[0m, in \u001B[0;36maccuracy\u001B[0;34m(model, dataloader)\u001B[0m\n\u001B[1;32m      6\u001B[0m cnt \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      7\u001B[0m acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     10\u001B[0m   inputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     11\u001B[0m   inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "Cell \u001B[0;32mIn[198], line 15\u001B[0m, in \u001B[0;36mcollate_fn\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m     12\u001B[0m texts, labels \u001B[38;5;241m=\u001B[39m [], []\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[1;32m     14\u001B[0m   \u001B[38;5;66;03m# 라벨을 감정이 아닌 각 텍스트의 뒷자리 단어(토큰) 2개로 변경\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m   tokenizer_output \u001B[38;5;241m=\u001B[39m tokenizer(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m], truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39mmax_len)\n\u001B[1;32m     16\u001B[0m   labels\u001B[38;5;241m.\u001B[39mappend(tokenizer_output\u001B[38;5;241m.\u001B[39minput_ids[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     17\u001B[0m   texts\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mLongTensor(tokenizer_output\u001B[38;5;241m.\u001B[39minput_ids[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3016\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   3014\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   3015\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 3016\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_one(text\u001B[38;5;241m=\u001B[39mtext, text_pair\u001B[38;5;241m=\u001B[39mtext_pair, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mall_kwargs)\n\u001B[1;32m   3017\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3018\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3126\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[1;32m   3105\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   3106\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3123\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3124\u001B[0m     )\n\u001B[1;32m   3125\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   3127\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   3128\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[1;32m   3129\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   3130\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3131\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   3132\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   3133\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m   3134\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m   3135\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m   3136\u001B[0m         padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m   3137\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m   3138\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m   3139\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m   3140\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m   3141\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m   3142\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m   3143\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m   3144\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m   3145\u001B[0m         split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[1;32m   3146\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3147\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3202\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   3192\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   3193\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   3194\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3195\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3199\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3200\u001B[0m )\n\u001B[0;32m-> 3202\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encode_plus(\n\u001B[1;32m   3203\u001B[0m     text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   3204\u001B[0m     text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[1;32m   3205\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   3206\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m   3207\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[1;32m   3208\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   3209\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m   3210\u001B[0m     is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m   3211\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m   3212\u001B[0m     padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m   3213\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m   3214\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m   3215\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m   3216\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m   3217\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m   3218\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m   3219\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m   3220\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m   3221\u001B[0m     split_special_tokens\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit_special_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_special_tokens),\n\u001B[1;32m   3222\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3223\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:603\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_encode_plus\u001B[39m(\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    581\u001B[0m     text: Union[TextInput, PreTokenizedInput],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    601\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchEncoding:\n\u001B[1;32m    602\u001B[0m     batched_input \u001B[38;5;241m=\u001B[39m [(text, text_pair)] \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;28;01melse\u001B[39;00m [text]\n\u001B[0;32m--> 603\u001B[0m     batched_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[1;32m    604\u001B[0m         batched_input,\n\u001B[1;32m    605\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m    606\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m    607\u001B[0m         padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m    608\u001B[0m         truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[1;32m    609\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m    610\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m    611\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m    612\u001B[0m         padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m    613\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m    614\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m    615\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m    616\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m    617\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m    618\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m    619\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m    620\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m    621\u001B[0m         split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[1;32m    622\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    623\u001B[0m     )\n\u001B[1;32m    625\u001B[0m     \u001B[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001B[39;00m\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_overflowing_tokens:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/hanghae99_plusAI_2/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:529\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[0m\n\u001B[1;32m    526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_special_tokens \u001B[38;5;241m!=\u001B[39m split_special_tokens:\n\u001B[1;32m    527\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_special_tokens \u001B[38;5;241m=\u001B[39m split_special_tokens\n\u001B[0;32m--> 529\u001B[0m encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_batch(\n\u001B[1;32m    530\u001B[0m     batch_text_or_text_pairs,\n\u001B[1;32m    531\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m    532\u001B[0m     is_pretokenized\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m    533\u001B[0m )\n\u001B[1;32m    535\u001B[0m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;66;03m# `Tokens` has type: Tuple[\u001B[39;00m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;66;03m#                       List[EncodingFast]\u001B[39;00m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[1;32m    541\u001B[0m tokens_and_encodings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_encoding(\n\u001B[1;32m    543\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[1;32m    553\u001B[0m ]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for data in train_loader:\n",
    "    model.zero_grad()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "    #preds = model(inputs)[..., 0] # 이진 분류에서 로짓 추출 (기존 손실 함수는 로짓을 원함)\n",
    "    preds = model(inputs)\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "  end_time = time.time()\n",
    "  epoch_time = end_time - start_time  # 에포크 실행 시간 계산\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1:3d} |\"\n",
    "    f\" Time: {epoch_time:.2f} seconds |\"\n",
    "    f\" Loss: {average_loss:.2f} |\"\n",
    "    f\" Train Acc: {train_acc:.3f} |\"\n",
    "    f\" Test Acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqZays2yb8Ja"
   },
   "source": [
    "학습이 안정적으로 진행되며 RNN보다 빨리 수렴하는 것을 확인할 수 있습니다.\n",
    "하지만 test 정확도가 RNN보다 낮은 것을 보았을 때, overfitting에 취약하다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAXB6GgIQy1S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
