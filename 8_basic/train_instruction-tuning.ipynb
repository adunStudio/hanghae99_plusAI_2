{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miamkimhongil92\u001b[0m (\u001b[33miamkimhongil92-lumenasoft\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/hanghae99_plusAI_2/8_basic/wandb/run-20250208_163505-5u51lj7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/5u51lj7o' target=\"_blank\">instruction-tuning</a></strong> to <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic' target=\"_blank\">https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/5u51lj7o' target=\"_blank\">https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/5u51lj7o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddeb332ac8e6428ca2808e33627346d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d88c857dab4602b4819974eb99e039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-00000-of-00001-e270777bb989ac86.parquet:   0%|          | 0.00/3.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c34279b63141ce84a631728557a981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d229fe9d9a48209246996b0dfa4abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe75416f9dd4d219af375be9f797ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727926b07c0d4927803df8bb1e6bb68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad648b5db8c94b41b55a0039ef7ee5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742e4bd2e3df461e9826e5bac41e8a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518cb50f299a43c2b3c781eb163c21df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad8a35bce624910a45815ab41cf35e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df067ecc6f754a4a9e06ed857bf651fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ac4673e27c4f20a6e8158ccd57c79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576e0df7f8b048479cd8f5373f74b090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12015' max='12015' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12015/12015 34:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.904500</td>\n",
       "      <td>1.626460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.567600</td>\n",
       "      <td>1.481831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.456800</td>\n",
       "      <td>1.362543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.383900</td>\n",
       "      <td>1.296385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.328100</td>\n",
       "      <td>1.249257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.287400</td>\n",
       "      <td>1.213777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.191200</td>\n",
       "      <td>1.192925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.159300</td>\n",
       "      <td>1.136221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>1.130851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>1.108425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>1.090299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.890700</td>\n",
       "      <td>1.080058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.914800</td>\n",
       "      <td>1.048996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>1.037997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.010418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.996428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>1.008012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.604100</td>\n",
       "      <td>1.002035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>0.994315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.600400</td>\n",
       "      <td>0.983934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.981133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>0.968517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.965480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.589600</td>\n",
       "      <td>0.960114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Alloc: 22.7 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(project='Hanghae99_8basic', name=f\"instruction-tuning\")\n",
    "\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ 90%:10% ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´ trainê³¼ validation ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "split_data = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = split_data['train']\n",
    "eval_dataset = split_data['test']\n",
    "\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "\n",
    "# 'formatting_prompts_func'ëŠ” ë°ì´í„°ì…‹ ì˜ˆì‹œë¥¼ ì…ë ¥ ë°›ì•„, 'Instruction'ê³¼ 'Output'ì„ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "# ê° 'Instruction'ê³¼ 'Output' ìŒì„ ì—°ê²°í•˜ì—¬ ëª¨ë¸ì´ ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "# ì£¼ì–´ì§„ í˜•ì‹: '### Question: [Instruction]\\n### Answer: [Output]'\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# ì½œë°± í´ë˜ìŠ¤ ì •ì˜\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # train loss ê¸°ë¡\n",
    "            if \"loss\" in logs:\n",
    "                wandb.log({\"train/loss\": logs[\"loss\"], \"step\": state.global_step})\n",
    "\n",
    "            # validation í‰ê°€ ë° loss ê¸°ë¡ (í‰ê°€ ì£¼ê¸°ì— ë”°ë¼ ì‹¤í–‰ë¨)\n",
    "            if \"eval_loss\" in logs:\n",
    "                wandb.log({\"eval/loss\": logs[\"eval_loss\"], \"step\": state.global_step})\n",
    "\n",
    "\n",
    "# TrainingArgumentsë¡œ ë¡œê·¸ ë¹ˆë„ ë° ê¸°íƒ€ í•™ìŠµ ì„¤ì • ê´€ë¦¬\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/clm-instruction-tuning\",  # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •\n",
    "    logging_steps=500,                         # ë¡œê·¸ ë¹ˆë„ ì„¤ì • (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ê¸°ë¡)\n",
    "    evaluation_strategy=\"steps\",               # í‰ê°€ ì „ëµì„ 'steps'ë¡œ ì„¤ì •\n",
    "    eval_steps=500,                            # í‰ê°€ ë¹ˆë„ ì„¤ì •\n",
    "    save_steps=0,                              # ì €ì¥ ë¹„í™œì„±í™”\n",
    "    save_total_limit=0,                        # ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ ì—†ìŒ\n",
    "    save_strategy=\"no\",                        # 'no'ë¡œ ì„¤ì •í•˜ì—¬ ì €ì¥ ì™„ì „ ë¹„í™œì„±í™”\n",
    "    per_device_train_batch_size=4,      # í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "    per_device_eval_batch_size=4        # í‰ê°€ ì‹œ ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    ")\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,                         # TrainingArgumentsë¡œ ì„¤ì • ì „ë‹¬\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    callbacks=[WandbLoggingCallback()]          # ì½œë°± ì¶”ê°€\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "max_memory_allocated_gb = round(torch.cuda.max_memory_allocated(0) / 1024**3, 1)\n",
    "print('Max Alloc:', max_memory_allocated_gb, 'GB')\n",
    "wandb.log({\"max_memory_allocated_gb\": max_memory_allocated_gb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"runtime\": 34 * 60 + 41})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–ˆâ–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–ƒâ–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–ƒâ–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>max_memory_allocated_gb</td><td>â–</td></tr><tr><td>runtime</td><td>â–</td></tr><tr><td>step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–…â–‚â–‚â–„â–…â–‚â–â–‚â–‚â–„â–â–ƒâ–‚â–„â–â–‚â–â–â–ƒâ–‚â–â–ƒâ–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–ˆâ–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.96011</td></tr><tr><td>eval/runtime</td><td>30.3191</td></tr><tr><td>eval/samples_per_second</td><td>132.095</td></tr><tr><td>eval/steps_per_second</td><td>33.048</td></tr><tr><td>max_memory_allocated_gb</td><td>22.7</td></tr><tr><td>runtime</td><td>2081</td></tr><tr><td>step</td><td>12000</td></tr><tr><td>total_flos</td><td>1.770568708472832e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>12015</td></tr><tr><td>train/grad_norm</td><td>8.10273</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5896</td></tr><tr><td>train_loss</td><td>0.9713</td></tr><tr><td>train_runtime</td><td>2081.6547</td></tr><tr><td>train_samples_per_second</td><td>23.083</td></tr><tr><td>train_steps_per_second</td><td>5.772</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">instruction-tuning</strong> at: <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/5u51lj7o' target=\"_blank\">https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/5u51lj7o</a><br> View project at: <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic' target=\"_blank\">https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250208_163505-5u51lj7o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
