{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miamkimhongil92\u001b[0m (\u001b[33miamkimhongil92-lumenasoft\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/hanghae99_plusAI_2/8_basic/wandb/run-20250208_162621-6cl9lwdd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/6cl9lwdd' target=\"_blank\">rank 128</a></strong> to <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic' target=\"_blank\">https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/6cl9lwdd' target=\"_blank\">https://wandb.ai/iamkimhongil92-lumenasoft/Hanghae99_8basic/runs/6cl9lwdd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b2ab79107b4c6c866ede197f1063e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd848cf6781f405cbcbd4a19eb1d59ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-00000-of-00001-e270777bb989ac86.parquet:   0%|          | 0.00/3.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26973357ac3040a8a67969d84274ec1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755618b1c99c45ec9da1199d6e2344d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7538d5fa26124f6ba09fc5e16766049f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ce485227b742b3b5eb6d88732f48e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459bf85ac17841739f6f759784f78523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d0feabefda4d39a147156ffdb21a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c0b18d9c3b4660add3ade4e7d5496b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2aac9b6c774d0489bc8a6efd2b7c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e664a2c083204bf2bdf3211354732db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0321c60b7e43b28efe04a4d63e04bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cbd6c737e24c879cc904fdc22e664f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12001' max='12015' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12001/12015 33:54 < 00:02, 5.90 it/s, Epoch 3.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.775100</td>\n",
       "      <td>1.542206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.612300</td>\n",
       "      <td>1.459686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.518500</td>\n",
       "      <td>1.413344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.488300</td>\n",
       "      <td>1.372355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.449300</td>\n",
       "      <td>1.344209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.431600</td>\n",
       "      <td>1.316130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.399200</td>\n",
       "      <td>1.299124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.382200</td>\n",
       "      <td>1.277490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.332500</td>\n",
       "      <td>1.261299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.328000</td>\n",
       "      <td>1.250090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.286400</td>\n",
       "      <td>1.239129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.288700</td>\n",
       "      <td>1.226139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.283700</td>\n",
       "      <td>1.216969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.291600</td>\n",
       "      <td>1.206756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.285200</td>\n",
       "      <td>1.200575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.239400</td>\n",
       "      <td>1.192733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.222600</td>\n",
       "      <td>1.187626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.223500</td>\n",
       "      <td>1.181519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.199600</td>\n",
       "      <td>1.177660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.214700</td>\n",
       "      <td>1.172946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.198500</td>\n",
       "      <td>1.170586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.184700</td>\n",
       "      <td>1.167468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.226400</td>\n",
       "      <td>1.164452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='361' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 361/1002 00:13 < 00:24, 26.08 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "lora_r: int = 128            # í–‰ë ¬ì˜ ë­í¬\n",
    "lora_dropout: float = 0.1  # LoRA parameterì— ì ìš©í•  dropout í™•ë¥ \n",
    "lora_alpha: int = 32       # LoRA parameterì¸ $A, B$ í–‰ë ¬ì„ scalingí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê°’\n",
    "\n",
    "wandb.init(project='Hanghae99_8basic', name=f\"rank {lora_r}\")\n",
    "\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ 90%:10% ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´ trainê³¼ validation ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "split_data = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = split_data['train']\n",
    "eval_dataset = split_data['test']\n",
    "\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "target_modules = set()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        names = name.split('.')\n",
    "        target_modules.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "if \"lm_head\" in target_modules:  # needed for 16-bit\n",
    "    target_modules.remove(\"lm_head\")\n",
    "\n",
    "target_modules = list(target_modules)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "# 'formatting_prompts_func'ëŠ” ë°ì´í„°ì…‹ ì˜ˆì‹œë¥¼ ì…ë ¥ ë°›ì•„, 'Instruction'ê³¼ 'Output'ì„ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "# ê° 'Instruction'ê³¼ 'Output' ìŒì„ ì—°ê²°í•˜ì—¬ ëª¨ë¸ì´ ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "# ì£¼ì–´ì§„ í˜•ì‹: '### Question: [Instruction]\\n### Answer: [Output]'\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# ì½œë°± í´ë˜ìŠ¤ ì •ì˜\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # train loss ê¸°ë¡\n",
    "            if \"loss\" in logs:\n",
    "                wandb.log({\"train/loss\": logs[\"loss\"], \"step\": state.global_step})\n",
    "\n",
    "            # validation í‰ê°€ ë° loss ê¸°ë¡ (í‰ê°€ ì£¼ê¸°ì— ë”°ë¼ ì‹¤í–‰ë¨)\n",
    "            if \"eval_loss\" in logs:\n",
    "                wandb.log({\"eval/loss\": logs[\"eval_loss\"], \"step\": state.global_step})\n",
    "\n",
    "\n",
    "# TrainingArgumentsë¡œ ë¡œê·¸ ë¹ˆë„ ë° ê¸°íƒ€ í•™ìŠµ ì„¤ì • ê´€ë¦¬\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/clm-instruction-tuning\",  # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •\n",
    "    logging_steps=500,                         # ë¡œê·¸ ë¹ˆë„ ì„¤ì • (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ê¸°ë¡)\n",
    "    evaluation_strategy=\"steps\",               # í‰ê°€ ì „ëµì„ 'steps'ë¡œ ì„¤ì •\n",
    "    eval_steps=500,                            # í‰ê°€ ë¹ˆë„ ì„¤ì •\n",
    "    save_steps=0,                              # ì €ì¥ ë¹„í™œì„±í™”\n",
    "    save_total_limit=0,                        # ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ ì—†ìŒ\n",
    "    save_strategy=\"no\",                        # 'no'ë¡œ ì„¤ì •í•˜ì—¬ ì €ì¥ ì™„ì „ ë¹„í™œì„±í™”\n",
    "    per_device_train_batch_size=4,      # í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "    per_device_eval_batch_size=4        # í‰ê°€ ì‹œ ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    ")\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,                         # TrainingArgumentsë¡œ ì„¤ì • ì „ë‹¬\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    callbacks=[WandbLoggingCallback()]          # ì½œë°± ì¶”ê°€\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "max_memory_allocated_gb = round(torch.cuda.max_memory_allocated(0) / 1024**3, 1)\n",
    "print('Max Alloc:', max_memory_allocated_gb, 'GB')\n",
    "wandb.log({\"max_memory_allocated_gb\": max_memory_allocated_gb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
