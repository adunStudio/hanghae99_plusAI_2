{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "from fontTools.misc.cython import returns\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "import datasets\n",
    "import argparse\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "from typing import Optional\n",
    "from itertools import chain\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# HuggingFace에서 데이터셋을 로드하기 위한 함수\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,           # 사전 학습된 모델의 설정을 자동으로 가져오기\n",
    "    AutoModelForCausalLM, # 사전 학습된 causal language model을 자동으로 가져오기\n",
    "    AutoTokenizer,        # 사전 학습된 토크나이저 자동 로드\n",
    "    HfArgumentParser,     # 커맨드 라인 인자를 파싱하기 위한 도구\n",
    "    Trainer,              # 훈련을 간소화하기 위한 HuggingFace Trainer 클래스\n",
    "    TrainingArguments,    # 훈련 설정 인자를 정의하는 클래스\n",
    "    default_data_collator # 데이터를 모델에 맞게 정렬하는 기본 데이터 collator\n",
    ")\n",
    "\n",
    "# 마지막 체크포인트 가져오는 함수\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 클래스 정의: 훈련에 필요한 인자들 설정\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    # HuggingFace hub에서 pre-trained 모델로 사용할 모델의 이름\n",
    "    model_name_or_path: Optional[str] = field(default=None)\n",
    "\n",
    "    # 우리 모델의 precision(data type이라고 이해하시면 됩니다)\n",
    "    torch_dtype: Optional[str] = field(default=None, metadata={'choices': ['auto', 'bfloat16', 'float16', 'float32']})\n",
    "\n",
    "    # Fine-tuning으로 사용할 huggingface hub에서의 dataset 이름\n",
    "    dataset_name: Optional[str] = field(default=None)\n",
    "\n",
    "    # Fine-tuning으로 사용할 huggingface hub에서의 dataset configuration\n",
    "    # 데이터셋의 구성 이름 (예: train/test/validation)\n",
    "    dataset_config_name: Optional[str] = field(default=None)\n",
    "\n",
    "    # Fine-tuning에 사용할 input text의 길이\n",
    "    # 텍스트를 나눌 블록의 크기\n",
    "    block_size: int = field(default=1024)\n",
    "\n",
    "    # Data를 업로드하거나 전처리할 때 사용할 worker 숫자\n",
    "    num_workers: Optional[int] = field(default=None)\n",
    "\n",
    "    # 로라 랭크\n",
    "    lora_r: int = field(default=8)\n",
    "\n",
    "\n",
    "\n",
    "# ArgumentParser를 사용하여 커맨드 라인 인자들을 파싱\n",
    "parser = HfArgumentParser((Arguments, TrainingArguments))\n",
    "args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "\n",
    "# Weights & Biases 프로젝트 초기화\n",
    "wandb.init(project='Hanghae99', name=f\"rank {args.lora_r}\")\n",
    "\n",
    "# 로깅 설정\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",  # 로그 출력 형식\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",  # 날짜 형식\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],  # 로그를 stdout으로 출력\n",
    ")\n",
    "\n",
    "# 로그 설정이 가능하면 INFO 레벨로 로깅\n",
    "if training_args.should_log:\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()  # INFO: 20\n",
    "\n",
    "# 우리가 가지고 있는 logger와 HuggingFace의 logger의 log level 설정\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "\n",
    "# 기타 HuggingFace logger option들을 설정\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# 훈련/평가 설정 출력\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "\n",
    "# 데이터셋 로드\n",
    "raw_datasets = load_dataset(\n",
    "    args.dataset_name,\n",
    "    args.dataset_config_name\n",
    ")\n",
    "\n",
    "# 모델, 토크나이저, 설정 파일 로드\n",
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "\n",
    "lora_r: int = args.lora_r  # 행렬의 랭크\n",
    "lora_dropout: float = 0.1  # LoRA parameter에 적용할 dropout 확률\n",
    "lora_alpha: int = 32       # LoRA parameter인 $A, B$ 행렬을 scaling할 때 사용하는 값\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype=args.torch_dtype\n",
    ")\n",
    "\n",
    "target_modules = set()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        names = name.split('.')\n",
    "        target_modules.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "if \"lm_head\" in target_modules:  # needed for 16-bit\n",
    "    target_modules.remove(\"lm_head\")\n",
    "\n",
    "target_modules = list(target_modules)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저의 pad 토큰을 eos 토큰으로 설정\n",
    "# <PAD>: 시퀀스의 길이를 맞추기 위한 패딩 토큰. 시퀀스들의 길이를 동일하게 맞춰주기 위해 추가되는 토큰이다.\n",
    "# <EOS>: 문장의 끝을 나타내는 토큰. 번역이나 생성 모델에서 사용되어 문장 생성의 종료를 표시한다.\n",
    "# 우리가 사용하는 tokenizer는 padding token이 없어서 추가해줍니다.\n",
    "# GPT 계열은 Casual LM으로 입력 시퀀스의 길이를 동적으로 처리하므로 PAD 토큰이 없다.\n",
    "# 이러한 경우 패딩이 필요한 상황에서는 pad_token을 명시적으로 설정해야 하며, 보통 eos_token을 패딩 역할로 사용한다.\n",
    "# -> (이는 tokenizer마다 다르니 유의)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 토큰 임베딩 크기를 토크나이저 크기에 맞게 조정\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 텍스트 컬럼 이름 설정\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰화하는 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[text_column_name])\n",
    "    return output\n",
    "\n",
    "\n",
    "# 데이터셋에 토큰화 함수 적용\n",
    "with training_args.main_process_first(desc=\"dataset map tokenization\"):\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,  # 배치 단위로 처리\n",
    "        num_proc=args.num_workers,  # 병렬 워커 수\n",
    "        remove_columns=column_names  # 토큰화 후 원래 컬럼 삭제\n",
    "    )\n",
    "\n",
    "\n",
    "# 최대 위치 임베딩 크기와 블록 크기 설정\n",
    "max_pos_embeddings = config.max_position_embeddings if hasattr(config, \"max_position_embeddings\") else 1024\n",
    "block_size = args.block_size if tokenizer.model_max_length is None else min(args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "# 텍스트들을 그룹화하는 함수 정의\n",
    "def group_texts(examples):\n",
    "    # 주어진 text들을 모두 concat 해줍니다.\n",
    "    # 예를 들어 examples = {'train': [['Hello!'], ['Yes, that is great!']]}이면 결과물은 {'train': ['Hello! Yes, that is great!']}가 됩니다.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}  # 모든 텍스트를 연결\n",
    "\n",
    "    # 전체 길이를 측정합니다.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])  # 전체 길이 계산\n",
    "    total_length = (total_length // block_size) * block_size  # 블록 크기에 맞춰 길이 조정\n",
    "\n",
    "\n",
    "    # 블록 단위로 텍스트를 분할\n",
    "    # 예를 들어 block_size=3일 때 {'train': ['Hello! Yes, that is great!']}는\n",
    "    # {'train': ['Hel', 'lo!', ' Ye', 's, ', 'tha', ...]}가 됩니다.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # 레이블은 입력 ID와 동일하게 설정\n",
    "    # Next token prediction이니 label은 자기 자신으로 설정합니다.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# 그룹화된 텍스트 데이터셋 생성\n",
    "with training_args.main_process_first(desc=\"grouping texts together\"):\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.num_workers\n",
    "    )\n",
    "\n",
    "\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# 콜백 클래스 정의\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        \"\"\"훈련 중 매 로그 이벤트 발생 시 호출됩니다.\"\"\"\n",
    "        if logs is not None:\n",
    "            # train loss 기록\n",
    "            if \"loss\" in logs:\n",
    "                wandb.log({\"train/loss\": logs[\"loss\"], \"step\": state.global_step})\n",
    "\n",
    "            # validation 평가 및 loss 기록 (평가 주기에 따라 실행됨)\n",
    "            if \"eval_loss\" in logs:\n",
    "                wandb.log({\"eval/loss\": logs[\"eval_loss\"], \"step\": state.global_step})\n",
    "\n",
    "\n",
    "# 학습 및 Validation 데이터셋 준비\n",
    "train_dataset      = lm_datasets[\"train\"]\n",
    "validation_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,  # validation 데이터셋 추가\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,  # 기본 데이터 collator 사용\n",
    "    callbacks = [WandbLoggingCallback()]  # 콜백 추가\n",
    ")\n",
    "\n",
    "# 체크포인트 설정\n",
    "checkpoint = None\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)  # 마지막 체크포인트 가져오기\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint  # 지정된 체크포인트에서 시작\n",
    "else:\n",
    "    checkpoint = last_checkpoint  # 마지막 체크포인트가 있으면 사용\n",
    "\n",
    "\n",
    "# 모델 훈련\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "# 최대 GPU 메모리 사용량을 계산\n",
    "max_memory_allocated_gb = round(torch.cuda.max_memory_allocated(0) / 1024**3, 1)\n",
    "print('Max Alloc:', max_memory_allocated_gb, 'GB')\n",
    "wandb.log({\"max_memory_allocated_gb\": max_memory_allocated_gb})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
