import json
from sklearn.model_selection import train_test_split
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset, DatasetDict
from transformers.trainer_utils import get_last_checkpoint
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM
from transformers import EarlyStoppingCallback
import evaluate
import wandb

# ğŸ”¹ 1. ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„
# corpus.json íŒŒì¼ ë¡œë“œ
with open("corpus.json", "r", encoding="utf-8") as file:
    corpus_data = json.load(file)

# ğŸ”¹ 2. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
train_data, val_data = train_test_split(corpus_data, test_size=0.2, random_state=42)

# ğŸ”¹ 3. ë°ì´í„°ì…‹ ìƒì„±
dataset = DatasetDict({
    "train": Dataset.from_list(train_data),
    "validation": Dataset.from_list(val_data)
})

# ğŸ”¹ 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •
model_name = "meta-llama/Llama-3.2-1B-Instruct"
#model_name = "google/gemma-2-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# ğŸ”¹ 5. ë°ì´í„° í¬ë§·íŒ… í•¨ìˆ˜
def formatting_prompts_func(examples):
    formatted_prompts = []

    for i, example in enumerate(examples):
        # í•˜ë‚˜ì˜ ì…ë ¥ì— ëŒ€í•´ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±
        input_data = examples["input"][i]
        output_terms = examples["output"][i]

        # inputì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        input_text = (
            f"### Title: {input_data['title']}\n"
            f"### Description: {input_data['description']}\n"
            f"### Summary: {input_data['summary']}\n"
            f"### í•µì‹¬ ìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n"
        )

        # outputì„ í‚¤í¬ì¸íŠ¸ë³„ë¡œ í¬ë§·íŒ…
        output_text = "\n".join(
            [f"- **{item['term']}**: {item['description']}" for item in output_terms]
        )

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ê²°í•©
        formatted_prompt = f"### Instruction:{input_text}\n### Response:\n{output_text}"

        # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        formatted_prompts.append(formatted_prompt)

    return formatted_prompts

# ğŸ”¹ 6. ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
response_template = "### Response:"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)

# ğŸ”¹ 7. W&B ì„¤ì •
wandb.init(project="llama-instruction-tuning", name="google/llama-2-2b-it")

# ğŸ”¹ 8. ROUGE ë° BLEU ë©”íŠ¸ë¦­ ë¡œë“œ
rouge_metric = evaluate.load("rouge")
bleu_metric = evaluate.load("bleu")

# ğŸ”¹ 9. í‰ê°€ í•¨ìˆ˜ ì •ì˜
def compute_metrics(eval_preds):
    predictions, labels = eval_preds
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ğŸ”¸ BLEU ì ìˆ˜ ê³„ì‚°
    predictions = [pred.split() for pred in decoded_preds]
    references = [[label.split()] for label in decoded_labels]
    bleu = bleu_metric.compute(predictions=predictions, references=references)

    # ğŸ”¸ ROUGE ì ìˆ˜ ê³„ì‚°
    rouge = rouge_metric.compute(
        predictions=decoded_preds,
        references=decoded_labels,
        use_stemmer=True
    )

    # ğŸ”¸ W&Bë¡œ ë©”íŠ¸ë¦­ ê¸°ë¡
    wandb.log({
        "bleu_score": bleu["bleu"],
        "rouge1": rouge["rouge1"].mid.fmeasure,
        "rouge2": rouge["rouge2"].mid.fmeasure,
        "rougeL": rouge["rougeL"].mid.fmeasure,
    })

    return {
        "bleu_score": bleu["bleu"],
        "rouge1": rouge["rouge1"].mid.fmeasure,
        "rouge2": rouge["rouge2"].mid.fmeasure,
        "rougeL": rouge["rougeL"].mid.fmeasure,
    }

#output_dir = "./finetuned_llama"
output_dir = "./finetuned_llama"

# ğŸ”¹ 10. SFT Trainer ì„¤ì •
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    args=SFTConfig(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        num_train_epochs=10,
        logging_dir="./logs",
        logging_steps=100,
        save_steps=100,
        load_best_model_at_end=True,  #
        metric_for_best_model="bleu_score",
        evaluation_strategy="steps",
        save_total_limit=2,
    ),
    formatting_func=formatting_prompts_func,
    data_collator=collator,
    compute_metrics=compute_metrics,  # ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # ğŸ”¹ ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì¶”ê°€
)


trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))

# ğŸ”¹ 11. í•™ìŠµ ì‹œì‘
last_checkpoint = get_last_checkpoint(output_dir)

if last_checkpoint is None:
    trainer.train()
else:
    trainer.train(resume_from_checkpoint=last_checkpoint)


wandb.finish()