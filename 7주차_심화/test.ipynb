{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T15:14:04.938577Z",
     "start_time": "2025-02-05T15:14:01.573862Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "# `load_dataset` 함수로 CodeAlpaca-20k 데이터셋을 로드합니다.\n",
    "# 이 데이터셋은 'Instruction'에 대한 'Output'을 제공하는 코드 생성 학습 데이터입니다.\n",
    "dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "\"\"\"\n",
    "- **Instruction**: 모델에게 주어지는 \"질문\" 또는 \"가이드라인\"입니다. 사용자가 원하는 코드나 기능을 설명하는 문장입니다.\n",
    "- **Input(optional)**: 코드가 실행될 때 받는 입력입니다. 이 부분은 선택사항일 수 있으며, 필요한 경우에만 포함됩니다.\n",
    "- **Output**: 사용자가 제시한 'Instruction'에 맞춰 모델이 생성해야 하는 'Answer'입니다. 이는 코드 출력이나 동작을 설명하는 부분입니다.\n",
    "\"\"\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "# 'facebook/opt-350m'은 OpenAI GPT 스타일의 언어 모델을 포함하는 사전 학습된 모델입니다.\n",
    "# Causal Language Modeling을 위한 모델로, 텍스트 생성 작업에 적합합니다.\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "\n",
    "# 'formatting_prompts_func'는 데이터셋 예시를 입력 받아, 'Instruction'과 'Output'을 적절한 형식으로 변환합니다.\n",
    "# 각 'Instruction'과 'Output' 쌍을 연결하여 모델이 이를 처리할 수 있도록 합니다.\n",
    "# 주어진 형식: '### Question: [Instruction]\\n### Answer: [Output]'\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "# 모델이 생성한 답변에 대한 템플릿을 정의합니다.\n",
    "# 모델이 답변을 생성하는 부분을 표기합니다. '### Answer:'는 모델이 이 위치에서 답변을 시작할 것임을 나타냅니다.\n",
    "response_template = \" ### Answer:\"\n",
    "\n",
    "# 'DataCollatorForCompletionOnlyLM'은 데이터 로딩 시 모델의 요구에 맞게 데이터를 형성하는 데 사용됩니다.\n",
    "# 주로 텍스트의 길이를 일정하게 맞추거나, 패딩을 추가하는 등의 역할을 합니다.\n",
    "# 여기서 템플릿인 'response_template'을 사용하여 모델이 답변을 생성하는 위치를 지정합니다.\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# SFTTrainer는 'Instruction-Tuning'을 위한 훈련 프로세스를 설정하는 클래스입니다.\n",
    "# 이 클래스는 `SFTConfig`와 함께 사용되어 모델을 훈련하고 결과를 저장하는 데 필요한 설정을 포함합니다.\n",
    "# 훈련을 위해 'train_dataset'을 사용하며, 'formatting_func'로 정의된 함수는 입력 데이터를 처리하고,\n",
    "# 'data_collator'는 데이터 배치 처리 작업을 담당합니다.\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    args=SFTConfig(output_dir=\"/tmp/clm-instruction-tuning\"),\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# `trainer.train()`을 호출하여 훈련을 시작합니다.\n",
    "# 모델은 데이터셋을 사용하여 'Instruction-Tuning'을 학습하게 됩니다.\n",
    "# 여기서 'Instruction-Tuning'은 모델이 주어진 'Instruction'에 맞춰 코드를 생성하는 능력을 향상시키는 훈련입니다.\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/677 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec8b61a3a41841bc9d68a82209f22cee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(…)-00000-of-00001-e270777bb989ac86.parquet:   0%|          | 0.00/3.45M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "835f69643f0d423abf686c016865d067"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/20022 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9d51a9004494ef88660be8cd8d69fc1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "161edebdfba44f8982ce4154d96a117e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dcfb3fcf72a4f48a27baa2d3f3d0b66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bfbba6d32c64efb95f0d38b67537ad4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "553d5fe0976142e3b73dea5f88c9a6e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "468765c38372402ba6d3d076dbcf1d6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "794ed556755e4cfc9718a3f137e9a2df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89c74e5e534d4c5eb7c706cf06e407e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/20022 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e597e8fbea94f3aa3c74405d1cba357"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='7509' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/7509 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "Process Process-auto_conversion:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/site-packages/transformers/safetensors_conversion.py\", line 101, in auto_conversion\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 860, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1009, in _hf_hub_download_to_cache_dir\n",
      "    _download_to_tmp_and_move(\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1543, in _download_to_tmp_and_move\n",
      "    http_get(\n",
      "  File \"/opt/miniconda3/envs/hanghae99/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 455, in http_get\n",
      "    temp_file.write(chunk)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
