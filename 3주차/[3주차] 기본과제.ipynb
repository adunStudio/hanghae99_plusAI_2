{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [3주차] 기본과제: DistilBERT로 뉴스 기사 분류 모델 학습하기"
   ],
   "metadata": {
    "id": "sbgz49PvHhLt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [MY CODE] AG_News dataset 준비\n",
    "## ✅ Huggingface dataset의 fancyzhx/ag_news를 load합니다.\n",
    "## ✅ Truncation과 관련된 부분들을 지웁니다."
   ],
   "metadata": {
    "id": "Cvfl_uFLIMWO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 1. 필요한 라이브러리 임포트\n",
    "from datasets import load_dataset\n",
    "\n",
    "###################################################################################################\n",
    "# ✅ Huggingface dataset의 fancyzhx/ag_news를 load합니다.\n",
    "###################################################################################################\n",
    "# 2. 데이터셋 로드\n",
    "ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "# ds는 일반적으로 DatasetDict 형태로, { \"train\": Dataset, \"test\": Dataset } 같은 스플릿을 가질 수 있음.\n",
    "\n",
    "# 3. 데이터셋의 전반적 정보 확인\n",
    "# print(ds)는 스플릿 이름, 샘플 수, 컬럼 등을 요약해서 보여줌\n",
    "print(\"\\n=== Dataset Info ===\")\n",
    "print(ds)\n",
    "\n",
    "# 4. 구체적으로 train 스플릿의 정보를 보고 싶다면\n",
    "train_ds = ds[\"train\"]\n",
    "print(\"\\n=== Train Dataset Info ===\")\n",
    "print(train_ds)\n",
    "\n",
    "# 5. train 스플릿의 크기(행 개수) 확인\n",
    "print(\"\\n=== Train Dataset Length(행 개수) ===\")\n",
    "print(len(train_ds))\n",
    "\n",
    "# 6. train 스플릿의 컬럼 이름, 타입, 라벨 항목 등을 확인\n",
    "# features 객체에는 각 컬럼의 스키마가 정의되어 있음\n",
    "print(\"\\n=== Train Dataset Features ===\")\n",
    "print(train_ds.features)\n",
    "\n",
    "# 7. 라벨(=카테고리) 목록 확인\n",
    "# 'label'이라는 컬럼이 ClassLabel 타입이라면 .names 로 전체 라벨명을 확인 가능\n",
    "if \"label\" in train_ds.features and hasattr(train_ds.features[\"label\"], \"names\"):\n",
    "    label_names = train_ds.features[\"label\"].names\n",
    "    print(\"\\n=== Label Names ===\")\n",
    "    print(label_names)\n",
    "\n",
    "# 8. 실제 데이터 예시 확인\n",
    "# 첫 번째 샘플을 보려면 아래처럼 사용 (keys: 컬럼명)\n",
    "print(\"\\n=== First sample of train dataset ===\")\n",
    "print(train_ds[0])\n",
    "\n",
    "# 9. 특정 컬럼들만 보고 싶다면\n",
    "print(\"\\n=== Just text and label of the first sample ===\")\n",
    "print({\n",
    "    \"text\": train_ds[0][\"text\"],\n",
    "    \"label\": train_ds[0][\"label\"]\n",
    "})\n",
    "\n",
    "# 10. DistilBERT 기반 토크나이저 로드\n",
    "#     huggingface/pytorch-transformers 레포지토리에서 'distilbert-base-uncased' 토크나이저를 불러옴\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')\n",
    "\n",
    "# 11. 배치를 텐서 형태로 변환해줄 collate_fn 함수 정의\n",
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(row['label'])\n",
    "    texts.append(row['text'])\n",
    "\n",
    "  # 텍스트 리스트를 토크나이저로 처리: 정수 인덱스 시퀀스로 변환\n",
    "  # - padding=True : 가장 긴 시퀀스 길이에 맞춰 패딩\n",
    "  # - truncation=True : 지정한 max_length를 초과하면 잘라냄\n",
    "  # - max_length=max_len : 최대 토큰 길이, None이면 trucation 무시\n",
    "  # .input_ids: 실제 토큰화된 결과(정수 리스트)를 반환\n",
    "  ###################################################################################################\n",
    "  # ✅ Truncation과 관련된 부분들을 지웁니다.\n",
    "  ###################################################################################################\n",
    "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=False, max_length=None).input_ids)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "# 12. PyTorch DataLoader 임포트\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 13. 훈련용 DataLoader 생성\n",
    "#     - ds['train']: 훈련 데이터셋\n",
    "#     - batch_size=64 : 한 배치에 64개 샘플씩 불러옴\n",
    "#     - shuffle=True : 매 에폭마다 데이터를 섞어 순서가 달라지게 함\n",
    "#     - collate_fn=collate_fn : 배치를 텐서 형태로 변환하는 함수를 지정\n",
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 14. 테스트용 DataLoader 생성\n",
    "#     - ds['test']: 테스트 데이터셋\n",
    "#     - batch_size=64 : 한 배치에 64개 샘플\n",
    "#     - shuffle=False : 테스트 시에는 보통 순서를 섞지 않음\n",
    "#     - collate_fn=collate_fn : 동일한 collate_fn 사용\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "id": "rE-y8sY9HuwP"
   },
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Info ===\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n",
      "\n",
      "=== Train Dataset Info ===\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 120000\n",
      "})\n",
      "\n",
      "=== Train Dataset Length(행 개수) ===\n",
      "120000\n",
      "\n",
      "=== Train Dataset Features ===\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n",
      "\n",
      "=== Label Names ===\n",
      "['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "=== First sample of train dataset ===\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "\n",
      "=== Just text and label of the first sample ===\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimhongil/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
   ],
   "metadata": {
    "id": "bF34XkoYIeEm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJaUp2Vob0U-",
    "outputId": "4cabca2b-06ce-480c-d52a-1381a955464b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimhongil/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": "DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "model\n",
    "\n",
    "# DistilBERT 모델 구조 분석\n",
    "# DistilBERT는 BERT의 경량화 버전으로, BERT의 12개 레이어 중 절반인 6개 레이어만 사용해 추론 속도는 약 60% 향상\n",
    "# 파라미터 수는 약 66M으로, BERT Base(110M)보다 적지만 GLUE 벤치마크 기준 약 95%의 성능 유지\n",
    "\n",
    "# 1. Embeddings Layer\n",
    "# - word_embeddings: 30522개의 토큰(단어)을 768차원의 벡터로 임베딩\n",
    "# - position_embeddings: 최대 512개의 위치 정보(문장 길이)를 임베딩해 문장 내 위치 정보 반영\n",
    "# - LayerNorm: 임베딩 결과를 정규화해 안정성 향상\n",
    "# - Dropout: 과적합 방지를 위해 10% 확률로 드롭아웃 적용\n",
    "#\n",
    "# 예시: Embedding(30522, 768) -> 30522개의 단어를 768차원으로 변환\n",
    "#       Embedding(512, 768) -> 문장 내 최대 512개 위치를 768차원으로 변환\n",
    "#\n",
    "# output: (batch_size, sequence_length, hidden_size=768)\n",
    "#\n",
    "# 예: 입력 토큰 시퀀스 -> 임베딩 -> (B, L, 768)\n",
    "\n",
    "# 2. Transformer Layer\n",
    "# - 6개의 Transformer Block 사용 (BERT의 절반)\n",
    "# - 각 블록은 Self-Attention 및 Feed Forward Network(FFN)으로 구성\n",
    "# - Multi-Head Self-Attention: 768차원의 입력에 대해 쿼리(Q), 키(K), 값(V) 연산\n",
    "# - Linear(768 -> 768): 어텐션 결과를 다시 768차원으로 변환\n",
    "# - LayerNorm: Self-Attention 후 정규화 적용\n",
    "#\n",
    "# MultiHeadSelfAttention:\n",
    "# - q_lin, k_lin, v_lin: Linear(768, 768)\n",
    "# - out_lin: Linear(768, 768)\n",
    "#\n",
    "# Feed Forward Network(FFN):\n",
    "# - lin1: Linear(768 -> 3072)  # 차원 확장 (4배 증가)\n",
    "# - lin2: Linear(3072 -> 768)  # 다시 768차원으로 축소\n",
    "# - 활성화 함수: GELU(Gaussian Error Linear Unit) 사용\n",
    "# - Dropout: FFN에도 드롭아웃 10% 적용\n",
    "#\n",
    "# output: (batch_size, sequence_length, hidden_size=768)\n",
    "\n",
    "# 3. 주요 하이퍼파라미터\n",
    "# - hidden_size = 768: 모든 임베딩 및 레이어 출력 차원\n",
    "# - intermediate_size = 3072: FFN의 확장 차원 (hidden_size의 4배)\n",
    "# - num_hidden_layers = 6: 총 6개의 Transformer 레이어\n",
    "# - num_attention_heads = 12: 12개의 어텐션 헤드 사용\n",
    "# - dropout = 0.1: Dropout 확률 10%\n",
    "\n",
    "# 4. 성능 및 비교\n",
    "# - BERT Base 대비 40% 가벼우며, 속도는 약 60% 향상\n",
    "# - GLUE 벤치마크에서 BERT의 95% 성능 유지\n",
    "# - 추론이 빠르고 메모리 효율이 높아 실시간 어플리케이션에 적합\n",
    "\n",
    "# 5. DistilBERT vs BERT 비교\n",
    "# | 항목                  | BERT Base           | DistilBERT        |\n",
    "# |-----------------------|---------------------|-------------------|\n",
    "# | 레이어 수             | 12                  | 6                 |\n",
    "# | 파라미터 수           | 110M                | 66M               |\n",
    "# | 임베딩 차원           | 768                 | 768               |\n",
    "# | FFN 차원              | 3072                | 3072              |\n",
    "# | 추론 속도             | 느림                | 약 60% 더 빠름     |\n",
    "# | 성능 (GLUE)           | 100%                | 95%               |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "출력 결과를 통해 우리는 DistilBERT의 architecture는 일반적인 Transformer와 동일한 것을 알 수 있습니다.\n",
    "Embedding layer로 시작해서 여러 layer의 Attention, FFN를 거칩니다.\n",
    "\n",
    "이제 DistilBERT를 거치고 난 `[CLS]` token의 representation을 가지고 text 분류를 하는 모델을 구현합시다."
   ],
   "metadata": {
    "id": "uh-tqY8WInQt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [MY CODE] Classifier output, loss function, accuracy function 변경\n",
    "## ✅ TextClassifier의 출력 차원을 잘 조정"
   ],
   "metadata": {
    "id": "_hFvSis0JLju"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "uyTciaPZ0KYo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimhongil/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self, num_classes=4):\n",
    "    super().__init__()\n",
    "\n",
    "    # DistilBERT 모델 로드\n",
    "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
    "\n",
    "    ###################################################################################################\n",
    "    # ✅ TextClassifier의 출력 차원을 잘 조정\n",
    "    ###################################################################################################\n",
    "    # 분류기: DistilBERT hidden size(768) -> 다중 분류 클래스 수(4)\n",
    "    self.classifier = nn.Linear(768, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # DistilBERT 인코더를 통과하여 hidden state 추출\n",
    "    # last_hidden_state는 최종 인코더에서 문장 내 모든 토큰에 대한 임베딩 정보\n",
    "    x = self.encoder(x)['last_hidden_state']\n",
    "\n",
    "    # 첫 번째 토큰([CLS])을 가져와 분류기에 전달\n",
    "    x = self.classifier(x[:, 0])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "model = TextClassifier()\n",
    "\n",
    "\"\"\"\n",
    "param.requires_grad = False를 통해 model.encoder(DistilBERT 인코더)의 모든 파라미터가\n",
    "역전파(Gradient Descent)에서 업데이트되지 않도록 설정한다.\n",
    "\n",
    " -> 전이 학습(Transfer Learning)\n",
    "\n",
    " 주요 의도\n",
    "\t1.\t사전 학습된 DistilBERT 인코더를 그대로 사용하고, 분류기(classifier) 부분만 학습합니다.\n",
    "        사전 학습된 언어 모델의 정보는 그대로 유지하고, 텍스트 분류기를 학습시키는 방식입니다.\n",
    "\t    새로운 데이터셋에 맞게 **분류기 헤드(FC 레이어)**만 조정하면 빠르게 성능을 확보할 수 있습니다.\n",
    "\t2.\t학습 속도 및 자원 절약\n",
    "\t    DistilBERT의 인코더는 수백만 개의 파라미터로 이루어져 있으며, 이를 업데이트하는 데 많은 시간이 걸립니다.\n",
    "\t    인코더를 동결하면 계산량이 크게 줄어 학습 속도가 빨라지고, 메모리 사용량이 절감됩니다.\n",
    "\t3.\t과적합 방지\n",
    "\t    작은 데이터셋에서 전체 모델을 학습시키면 과적합(overfitting)될 가능성이 높습니다.\n",
    "\t    인코더를 동결하고, 분류기만 학습하면 일반화 성능이 더 높아질 수 있습니다.\n",
    "\"\"\"\n",
    "for param in model.encoder.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ accuracy function 변경"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    preds = model(inputs)\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "    #preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt\n",
    "\n",
    "def plot_acc(train_accuracies, test_accuracies, label1='train', label2='test'):\n",
    "  x = np.arange(len(train_accuracies))\n",
    "\n",
    "  plt.plot(x, train_accuracies, label=label1)\n",
    "  plt.plot(x, test_accuracies, label=label2)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터셋 최대 문장 길이: 379\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for batch in train_loader:\n",
    "    lengths = [len(text) for text in batch[0]]\n",
    "    batch_max = max(lengths)\n",
    "    max_len = max(max_len, batch_max)\n",
    "\n",
    "print(f\"Train 데이터셋 최대 문장 길이: {max_len}\")\n",
    "# 배치 내 문장이 자동으로 가장 긴 문장에 맞게 패딩됩니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "=========================================================================================================\nLayer (type:depth-idx)                                  Output Shape              Param #\n=========================================================================================================\nTextClassifier                                          [64, 4]                   --\n├─DistilBertModel: 1-1                                  [64, 512, 768]            --\n│    └─Embeddings: 2-1                                  [64, 512, 768]            --\n│    │    └─Embedding: 3-1                              [64, 512, 768]            (23,440,896)\n│    │    └─Embedding: 3-2                              [1, 512, 768]             (393,216)\n│    │    └─LayerNorm: 3-3                              [64, 512, 768]            (1,536)\n│    │    └─Dropout: 3-4                                [64, 512, 768]            --\n│    └─Transformer: 2-2                                 [64, 512, 768]            --\n│    │    └─ModuleList: 3-5                             --                        (42,527,232)\n├─Linear: 1-2                                           [64, 4]                   3,076\n=========================================================================================================\nTotal params: 66,365,956\nTrainable params: 3,076\nNon-trainable params: 66,362,880\nTotal mult-adds (Units.GIGABYTES): 4.22\n=========================================================================================================\nInput size (MB): 0.26\nForward/backward pass size (MB): 13693.36\nParams size (MB): 265.46\nEstimated Total Size (MB): 13959.08\n========================================================================================================="
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# 모델 정보 출력\n",
    "summary(model, input_size=(64, 512), dtypes=[torch.int64])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# 1. CUDA(엔비디아 GPU) -> 2. MPS(Apple GPU) -> 3. CPU 순서로 디바이스 선택\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [MY CODE] 훈련\n",
    "## Save/Load\n",
    "## ✅ nn.CrossEntropyLoss 를 추가"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvvaAEwCznt-",
    "outputId": "3363b8ca-7695-493f-96a0-5aa6b52d1d60",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새롭게 시작~\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 10\n",
    "start_epoch = 0\n",
    "\n",
    "time_list = []\n",
    "average_loss_list = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "checkpoint_path = '3_checkpoint_TextClassifier.pth'\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']  # 이어서 시작할 에포크\n",
    "    time_list = checkpoint['time_list']\n",
    "    average_loss_list = checkpoint['average_loss_list']\n",
    "    train_accuracies = checkpoint['train_accuracies']\n",
    "    test_accuracies = checkpoint['test_accuracies']\n",
    "    for epoch in range(0, start_epoch):\n",
    "        print(f\"Epoch {epoch+1:3d} |\"\n",
    "        f\" Time: {time_list[epoch]:.2f} seconds |\"\n",
    "        f\" Loss: {average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Train Acc: {train_accuracies[epoch]:.3f} |\"\n",
    "        f\" Test Acc: {test_accuracies[epoch]:.3f}\")\n",
    "\n",
    "    if start_epoch < n_epochs -1:\n",
    "        print(f\"이어서 시작~ {start_epoch + 1}.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"새롭게 시작~\")\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        model.zero_grad()\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        preds = model(inputs)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time  # 에포크 실행 시간 계산\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_loader)\n",
    "        test_acc = accuracy(model, test_loader)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        time_list.append(epoch_time)\n",
    "        average_loss_list.append(average_loss)\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'time_list': time_list,\n",
    "        'average_loss_list': average_loss_list,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:3d} |\"\n",
    "    f\" Time: {epoch_time:.2f} seconds |\"\n",
    "    f\" Loss: {average_loss:.2f} |\"\n",
    "    f\" Train Acc: {train_acc:.3f} |\"\n",
    "    f\" Test Acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjphVwXL00E2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7526ec71-f015-4f26-8035-3091ed71869e"
   },
   "outputs": [],
   "source": [
    "plot_acc(train_accuracies, test_accuracies)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
