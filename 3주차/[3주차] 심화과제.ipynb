{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [3주차] 심화과제: Machine translation(기계 번역)"
   ],
   "metadata": {
    "id": "sbgz49PvHhLt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# !pip install tqdm boto3 requests regex sentencepiece sacremoses datasets safetensors transformers tokenizers matplotlib torchinfo tqdm sacrebleu pandas scikit-learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [MY CODE] Language Translation (English-French) dataset 준비"
   ],
   "metadata": {
    "id": "Cvfl_uFLIMWO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 1. 데이터 불러오기 & 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175621, 2)\n",
      "Index(['English words/sentences', 'French words/sentences'], dtype='object')\n",
      "  English words/sentences French words/sentences\n",
      "0                     Hi.                 Salut!\n",
      "1                    Run!                Cours !\n",
      "2                    Run!               Courez !\n",
      "3                    Who?                  Qui ?\n",
      "4                    Wow!             Ça alors !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eng_french_data = pd.read_csv('eng_-french.csv')\n",
    "print(eng_french_data.shape)\n",
    "print(eng_french_data.columns)\n",
    "print(eng_french_data.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 2. 훈련/테스트 셋 분리 (Train/Test Split)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: 140496\n",
      "테스트 데이터 크기: 35125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련 데이터: 80%, 테스트 데이터: 20%\n",
    "train_data, test_data = train_test_split(eng_french_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"훈련 데이터 크기: {len(train_data)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_data)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 3. T5 토크나이저 준비 & 토크나이징"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')  # t5-small, t5-base, t5-large\n",
    "\n",
    "english_column = 'English words/sentences'\n",
    "french_column  = 'French words/sentences'\n",
    "\n",
    "# 훈련 및 테스트 데이터 토크나이징\n",
    "train_encodings = tokenizer(list(train_data[english_column]), padding=True, truncation=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_data[english_column]), padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# 라벨(프랑스어) 토크나이징\n",
    "train_labels = tokenizer(list(train_data[french_column]), padding=True, truncation=True, max_length=512)\n",
    "test_labels = tokenizer(list(test_data[french_column]), padding=True, truncation=True, max_length=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 4. 데이터셋 클래스로 변환 (PyTorch Dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels['input_ids'][idx],\n",
    "        }\n",
    "\n",
    "train_dataset = TranslationDataset(train_encodings, train_labels)\n",
    "test_dataset = TranslationDataset(test_encodings, test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 5. DataLoader 준비"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# collate_fn 정의\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sacrebleu\n",
    "\n",
    "def accuracy(model, dataloader, tokenizer):\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # T5는 generate()를 사용해 예측 시퀀스를 생성\n",
    "            preds = model.generate(input_ids=input_ids, max_length=labels.size(1))\n",
    "\n",
    "            # 토큰 -> 텍스트로 변환 후 비교 (정확도 측정)\n",
    "            pred_texts = [tokenizer.decode(p, skip_special_tokens=True) for p in preds]\n",
    "            label_texts = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]\n",
    "\n",
    "            # 문장 단위 비교 (정확히 일치하는 문장만 정답으로 간주)\n",
    "            for p, l in zip(pred_texts, label_texts):\n",
    "                if p == l:\n",
    "                    acc += 1\n",
    "                cnt += 1\n",
    "\n",
    "    return acc / cnt\n",
    "\n",
    "\n",
    "\n",
    "# BLEU 점수 계산 함수 추가\n",
    "def calculate_bleu(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model.generate(input_ids=input_ids, max_length=labels.size(1))\n",
    "\n",
    "            pred_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            ref_texts = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
    "\n",
    "            predictions.extend(pred_texts)\n",
    "            references.extend(ref_texts)\n",
    "\n",
    "    bleu_score = sacrebleu.corpus_bleu(predictions, [references]).score\n",
    "    return bleu_score\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    acc = 0\n",
    "    cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 예측 생성\n",
    "            preds = model.generate(input_ids=input_ids, max_length=labels.size(1))\n",
    "\n",
    "            # 토큰 -> 텍스트 변환\n",
    "            pred_texts = [tokenizer.decode(p, skip_special_tokens=True) for p in preds]\n",
    "            ref_texts = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]\n",
    "\n",
    "            # 정확도 계산 (문장 단위 비교)\n",
    "            for p, l in zip(pred_texts, ref_texts):\n",
    "                if p == l:\n",
    "                    acc += 1\n",
    "                cnt += 1\n",
    "\n",
    "            # BLEU 계산을 위한 데이터 축적\n",
    "            predictions.extend(pred_texts)\n",
    "            references.extend(ref_texts)\n",
    "\n",
    "    # BLEU 점수 계산\n",
    "    bleu_score = sacrebleu.corpus_bleu(predictions, [references]).score\n",
    "    accuracy = acc / cnt\n",
    "\n",
    "    return accuracy, bleu_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "    acc = 0\n",
    "    cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"Evaluating ...\")\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 1) Loss 계산\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 2) generate()로 텍스트 예측\n",
    "            preds = model.generate(input_ids=input_ids, max_length=labels.size(1))\n",
    "            pred_texts = [tokenizer.decode(p, skip_special_tokens=True) for p in preds]\n",
    "            ref_texts  = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]\n",
    "\n",
    "            # 3) Accuracy 계산\n",
    "            for p, r in zip(pred_texts, ref_texts):\n",
    "                if p == r:\n",
    "                    acc += 1\n",
    "                cnt += 1\n",
    "\n",
    "            # 4) BLEU 계산용 데이터\n",
    "            predictions.extend(pred_texts)\n",
    "            references.extend(ref_texts)\n",
    "\n",
    "    # 평균 Loss와 Perplexity\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    ppl = torch.exp(torch.tensor(avg_loss))\n",
    "\n",
    "    # BLEU 점수\n",
    "    bleu_score = sacrebleu.corpus_bleu(predictions, [references]).score\n",
    "    accuracy = acc / cnt\n",
    "\n",
    "    return avg_loss, ppl, accuracy, bleu_score\n",
    "\n",
    "def to_float_list(values):\n",
    "        \"\"\"\n",
    "        전달된 values가 None이면 None 반환.\n",
    "        list라면 각 요소를 float로 변환해 리스트로 반환.\n",
    "        (GPU Tensor인 경우 detach().cpu().item() 사용)\n",
    "        \"\"\"\n",
    "        if values is None:\n",
    "            return None\n",
    "\n",
    "        float_list = []\n",
    "        for v in values:\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                # 만약 v가 GPU 텐서라면 CPU로 이동 후 float 추출\n",
    "                float_list.append(float(v.detach().cpu().item()))\n",
    "            else:\n",
    "                # 이미 float 등의 형태라면 그대로 float 캐스팅\n",
    "                float_list.append(float(v))\n",
    "        return float_list\n",
    "\n",
    "def plot_acc(ax, title, train_accuracies, test_accuracies, label1='train', label2='test'):\n",
    "    x = np.arange(len(test_accuracies))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    train_accuracies = to_float_list(train_accuracies)\n",
    "    test_accuracies = to_float_list(test_accuracies)\n",
    "\n",
    "    if train_accuracies is not None:\n",
    "        ax.plot(x, train_accuracies, label=label1, color='red')\n",
    "        ax.annotate(f'{train_accuracies[-1]:.2f}',\n",
    "                    xy=(x[-1], train_accuracies[-1]),\n",
    "                    xytext=(5, 0),\n",
    "                    textcoords='offset points',\n",
    "                    color='red',\n",
    "                    ha='left', va='center')\n",
    "\n",
    "    if test_accuracies is not None:\n",
    "        ax.plot(x, test_accuracies, label=label2, color='lime')\n",
    "        ax.annotate(f'{test_accuracies[-1]:.2f}',\n",
    "                    xy=(x[-1], test_accuracies[-1]),\n",
    "                    xytext=(5, 0),\n",
    "                    textcoords='offset points',\n",
    "                    color='lime',\n",
    "                    ha='left', va='center')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def plot_acc3(ax, title, list1, list2, list3, label1='none-trained + Full Training', label2='pre-trained + Decoder Fine-tuning', label3='pre-trained + Full Fine-tuning'):\n",
    "    x = np.arange(len(list1))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    colors = ['gray', 'green', 'red']\n",
    "    linestyles = ['-', '--', '-.']\n",
    "    markers = ['o', 's', '^']\n",
    "\n",
    "    _list1 = to_float_list(list1)\n",
    "    _list2 = to_float_list(list2)\n",
    "    _list3 = to_float_list(list3)\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "\n",
    "    if _list1 is not None:\n",
    "        ax.plot(x, _list1, label=label1, color=colors[0], linestyle=linestyles[0], marker=markers[0], alpha=0.8)\n",
    "        ax.annotate(f'{_list1[-1]:.2f}',\n",
    "                    xy=(x[-1], _list1[-1]),\n",
    "                    xytext=(5, 0),\n",
    "                    textcoords='offset points',\n",
    "                    color=colors[0],\n",
    "                    ha='left', va='center')\n",
    "\n",
    "    if _list2 is not None:\n",
    "        ax.plot(x, _list2, label=label2, color=colors[1], linestyle=linestyles[1], marker=markers[1], alpha=0.8)\n",
    "        ax.annotate(f'{_list2[-1]:.2f}',\n",
    "                    xy=(x[-1], _list2[-1]),\n",
    "                    xytext=(5, 0),\n",
    "                    textcoords='offset points',\n",
    "                    color=colors[1],\n",
    "                    ha='left', va='center')\n",
    "\n",
    "    if _list3 is not None:\n",
    "        ax.plot(x, _list3, label=label3, color=colors[2], linestyle=linestyles[2], marker=markers[2], alpha=0.8)\n",
    "        ax.annotate(f'{_list3[-1]:.2f}',\n",
    "                    xy=(x[-1], _list3[-1]),\n",
    "                    xytext=(5, 0),\n",
    "                    textcoords='offset points',\n",
    "                    color=colors[2],\n",
    "                    ha='left', va='center')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [MY CODE] T5 모델\n",
    "## t5-small 모델 사용\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 모델 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: hello\n",
      "Translated Text (English → French): Hallo\n"
     ]
    }
   ],
   "source": [
    "## t5-small 모델 사용\n",
    "# 2. 번역할 문장 + Prompt\n",
    "text_to_translate = \"hello\"\n",
    "prompt = f\"translate English to French: {text_to_translate}\"\n",
    "\n",
    "# 3. 토크나이즈\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 4. 모델 추론 (generate)\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=40,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 5. 결과 디코딩\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 6. 출력\n",
    "print(\"Input Text:\", text_to_translate)\n",
    "print(\"Translated Text (English → French):\", translated_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# T5ForConditionalGeneration 모델 설명\n",
    "\n",
    "T5는 \"인코더-디코더(Seq2Seq)\" 구조의 모델로, 텍스트 입력을 **인코딩**한 뒤 이를 **디코더**가 참고하여 새로운 텍스트를 **생성**합니다.\n",
    "Hugging Face의 `T5ForConditionalGeneration` 클래스는 이 전체 구조(인코더+디코더+Language Model Head)를 한데 묶은 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) shared: Embedding(32128, 512)\n",
    "\n",
    "- **shared 임베딩**: T5는 인코더와 디코더가 **같은 단어 임베딩 테이블**을 공유합니다.\n",
    "- `32128`은 T5 토크나이저의 기본 **어휘 개수**(vocab size), `512`는 **히든 차원**(d_model)을 의미합니다.\n",
    "- 입력 토큰 ID(0~32127 범위)를 512차원 벡터로 변환합니다.\n",
    "- 이 레이어는 구현상 인코더·디코더 각각에 `embed_tokens`가 있긴 하지만, 내부적으로는 동일한 파라미터(`shared`)를 참조합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) encoder: T5Stack\n",
    "\n",
    "T5의 **인코더** 부분이며, 입력 시퀀스(예: 문장)를 **문맥적(hidden state)으로 변환**하는 역할을 합니다.\n",
    "\n",
    "### (a) embed_tokens: Embedding(32128, 512)\n",
    "\n",
    "- 인코더에서 사용하는 임베딩 레이어.\n",
    "- 실제로는 위의 **`shared` 임베딩**과 동일한 파라미터를 가리킵니다.\n",
    "- 입력으로 들어온 토큰 ID를 512차원 벡터로 매핑.\n",
    "\n",
    "### (b) block: ModuleList([...])  -- 총 6개의 T5Block\n",
    "\n",
    "- **T5-Small**은 인코더 레이어가 **6개**입니다. (T5-Base는 12개, T5-Large는 24개 등)\n",
    "- 각 인코더 블록(`T5Block`)은 크게 **두 부분**으로 구성:\n",
    "  1. **Self-Attention** (자기 자신에 대한 어텐션)\n",
    "  2. **Feed-Forward** (Dense -> ReLU -> Dense)\n",
    "\n",
    "#### └─ T5Block 내부 구조\n",
    "\n",
    "- `T5LayerSelfAttention`:\n",
    "  - `T5Attention`으로 구현된 **셀프 어텐션** 모듈입니다.\n",
    "    - (q, k, v, o): 512→512 **선형 변환**.\n",
    "    - `relative_attention_bias`: T5가 사용하는 **상대적 위치 정보**(Embedding(32, 8))를 통해, 단순 절대 위치 대신 상대 거리에 따른 attention bias를 학습합니다.\n",
    "  - `layer_norm`: 레이어 정규화\n",
    "  - `dropout(p=0.1)`: 드롭아웃 적용\n",
    "\n",
    "- `T5LayerFF`:\n",
    "  - **Feed-Forward** 부분(`DenseReluDense`)\n",
    "    - wi: Linear(512 → 2048)\n",
    "    - wo: Linear(2048 → 512)\n",
    "    - act: ReLU\n",
    "    - dropout(p=0.1)\n",
    "  - `layer_norm`: 레이어 정규화\n",
    "  - `dropout(p=0.1)`: 드롭아웃 적용\n",
    "\n",
    "> 인코더의 각 레이어는 “**(Self-Attn) → (Feed-Forward)**” 순서로 진행됩니다.\n",
    "\n",
    "### (c) final_layer_norm: T5LayerNorm()\n",
    "\n",
    "- 인코더 블록을 전부 지난 뒤, 마지막으로 출력 hidden state에 대한 레이어 정규화.\n",
    "\n",
    "### (d) dropout: Dropout(p=0.1)\n",
    "\n",
    "- 인코더 출력 단에 적용되는 드롭아웃(확률 0.1).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) decoder: T5Stack\n",
    "\n",
    "T5의 **디코더** 부분으로, 인코더 출력을 참고하면서 **새로운 시퀀스**(토큰)를 생성합니다.\n",
    "\n",
    "### (a) embed_tokens: Embedding(32128, 512)\n",
    "\n",
    "- 디코더 입력(이전까지 생성한 토큰, 또는 teacher forcing 시 레이블 토큰)을 512차원으로 매핑.\n",
    "- 마찬가지로 **shared 임베딩**과 동일 파라미터.\n",
    "\n",
    "### (b) block: ModuleList([...]) -- 총 6개의 T5Block\n",
    "\n",
    "- **T5-Small**에서 디코더도 **6개 레이어**를 가집니다.\n",
    "- **디코더 블록**은 인코더 블록과 달리,\n",
    "  1. Self-Attention\n",
    "  2. **Cross-Attention** (인코더-디코더 어텐션)\n",
    "  3. Feed-Forward\n",
    "  의 **3단 구조**를 가집니다.\n",
    "\n",
    "#### └─ T5Block 내부 (디코더용)\n",
    "\n",
    "1. **T5LayerSelfAttention**\n",
    "   - 디코더 **자기 자신**(이전 토큰)에 대한 어텐션.\n",
    "   - 통상적으로 **미래 토큰**은 가려지는(Causal Mask) 형태로 동작(“look-ahead mask”).\n",
    "   - `relative_attention_bias`를 통해 디코더 내에서도 상대적 위치정보 사용.\n",
    "\n",
    "2. **T5LayerCrossAttention** (EncDecAttention)\n",
    "   - 디코더가 **인코더** 출력(=Key,Value)에 대하여 어텐션을 수행.\n",
    "   - 디코더 hidden state가 Query가 되어, 인코더 hidden state(서로 다른 시퀀스 길이)를 K, V로 삼아 컨텍스트를 얻습니다.\n",
    "   - 이를 통해 번역이나 요약 등 **입력 문맥**을 참고하여 적절한 단어를 생성합니다.\n",
    "\n",
    "3. **T5LayerFF** (Feed-Forward)\n",
    "   - 인코더와 동일한 구조(`DenseReluDense`), 512→2048→512 + ReLU.\n",
    "\n",
    "- 각 서브 레이어마다 `layer_norm`, `dropout`이 적용됩니다.\n",
    "\n",
    "### (c) final_layer_norm: T5LayerNorm()\n",
    "\n",
    "- 디코더의 모든 레이어를 통과한 뒤, 최종적으로 레이어 정규화.\n",
    "\n",
    "### (d) dropout: Dropout(p=0.1)\n",
    "\n",
    "- 디코더 최종 출력에 대한 드롭아웃.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) lm_head: Linear(512 → 32128, bias=False)\n",
    "\n",
    "- 디코더 출력(최종 hidden state 512차원)을 **vocab 크기(32128)**로 매핑.\n",
    "- 각 타임스텝에서 **단어 분포**(로짓)를 산출하여, 소프트맥스하면 다음 토큰 확률을 얻게 됩니다.\n",
    "- T5는 일반적으로 출력층에 bias를 사용하지 않습니다(`bias=False`).\n",
    "\n",
    "---\n",
    "\n",
    "## 한눈에 보는 흐름\n",
    "\n",
    "1. **(shared) 임베딩**\n",
    "   - 입력 토큰 ID → (512차원 벡터)\n",
    "\n",
    "2. **인코더(encoder) 6개 레이어**\n",
    "   - (Self-Attention + Feed-Forward) x 6\n",
    "   - 최종적으로 “입력 문장 전체를 맥락적으로 요약한 **인코더 히든 스테이트**”를 출력\n",
    "\n",
    "3. **디코더(decoder) 6개 레이어**\n",
    "   - 각 레이어가 **Self-Attn**(이전 토큰들 참고) → **Cross-Attn**(인코더 히든 스테이트 참고) → **Feed-Forward**\n",
    "   - 순차적으로 토큰을 생성(학습 시 teacher forcing, 추론 시 오토리그레시브)\n",
    "\n",
    "4. **lm_head**\n",
    "   - 디코더 최종 출력을 **vocab** 크기로 변환해, **단어 분포**(로짓) 산출 → 다음 토큰 예측\n",
    "\n",
    "이렇게 T5는 “인코더-디코더 구조 + Shared Embedding + Relative Position Bias”가 핵심이며,\n",
    "다양한 시퀀스 변환 문제(번역, 요약, 문장 완성 등)를 통일된 방식(“text-to-text”)으로 처리할 수 있도록 설계되었습니다.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 간단 설명\n",
    "Shared Embedding: T5는 인코더/디코더에서 동일한 임베딩 테이블(shared)을 사용.\n",
    "\n",
    "Encoder:\n",
    "6개 블록, 각 블록은 Self-Attention과 Feed-Forward 레이어로 구성.\n",
    "입력 문장을 이해/인코딩.\n",
    "\n",
    "Decoder:\n",
    "6개 블록, 각 블록은 Self-Attention, Cross-Attention(인코더 정보 활용), Feed-Forward 레이어로 구성.\n",
    "인코더 출력 + 이전 디코딩 맥락을 기반으로 새로운 시퀀스를 생성.\n",
    "lm_head: 디코더 결과를 단어 확률 분포로 매핑해서 다음 토큰을 예측.\n",
    "\n",
    "이 구조가 인코더-디코더 기반으로 입력을 처리하고, 텍스트를 생성하는 T5의 전형적인 형태"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "=========================================================================================================\nLayer (type:depth-idx)                                  Output Shape              Param #\n=========================================================================================================\nT5ForConditionalGeneration                              [1, 128, 512]             --\n├─T5Stack: 1-1                                          [1, 128, 512]             35,330,816\n├─T5Stack: 1-2                                          --                        (recursive)\n│    └─Embedding: 2-1                                   [1, 128, 512]             16,449,536\n├─T5Stack: 1-3                                          --                        (recursive)\n│    └─Dropout: 2-2                                     [1, 128, 512]             --\n│    └─ModuleList: 2-3                                  --                        --\n│    │    └─T5Block: 3-1                                [1, 128, 512]             3,147,008\n│    │    └─T5Block: 3-2                                [1, 128, 512]             3,146,752\n│    │    └─T5Block: 3-3                                [1, 128, 512]             3,146,752\n│    │    └─T5Block: 3-4                                [1, 128, 512]             3,146,752\n│    │    └─T5Block: 3-5                                [1, 128, 512]             3,146,752\n│    │    └─T5Block: 3-6                                [1, 128, 512]             3,146,752\n│    └─T5LayerNorm: 2-4                                 [1, 128, 512]             512\n│    └─Dropout: 2-5                                     [1, 128, 512]             --\n├─T5Stack: 1-4                                          [1, 8, 128, 64]           16,449,536\n│    └─Embedding: 2-6                                   [1, 128, 512]             (recursive)\n│    └─Dropout: 2-7                                     [1, 128, 512]             --\n│    └─ModuleList: 2-8                                  --                        --\n│    │    └─T5Block: 3-7                                [1, 128, 512]             4,196,096\n│    │    └─T5Block: 3-8                                [1, 128, 512]             4,195,840\n│    │    └─T5Block: 3-9                                [1, 128, 512]             4,195,840\n│    │    └─T5Block: 3-10                               [1, 128, 512]             4,195,840\n│    │    └─T5Block: 3-11                               [1, 128, 512]             4,195,840\n│    │    └─T5Block: 3-12                               [1, 128, 512]             4,195,840\n│    └─T5LayerNorm: 2-9                                 [1, 128, 512]             512\n│    └─Dropout: 2-10                                    [1, 128, 512]             --\n├─Linear: 1-5                                           [1, 128, 32128]           16,449,536\n=========================================================================================================\nTotal params: 128,736,512\nTrainable params: 128,736,512\nNon-trainable params: 0\nTotal mult-adds (M): 93.47\n=========================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 122.03\nParams size (MB): 307.82\nEstimated Total Size (MB): 429.86\n========================================================================================================="
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# 모델 정보 출력\n",
    "# 더미 데이터 생성 (batch_size=2, sequence_length=128)\n",
    "dummy_input = {\n",
    "    \"input_ids\": torch.randint(0, 32128, (1, 128)),\n",
    "    \"attention_mask\": torch.ones(1, 128),\n",
    "    \"decoder_input_ids\": torch.randint(0, 32128, (1, 128))\n",
    "}\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "summary(model, input_data=dummy_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [MY CODE] T5 모델 학습 & 평가\n",
    "(총 4가지 버전)\n",
    "### 1. only pre-trained upstream\n",
    "### 2. none-trained + Full Training\n",
    "### 3. pre-trained + Decoder Fine-tuning (+lm_head)\n",
    "### 4. pre-trained + Full Fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 메트릭 딕셔너리 정의\n",
    "metric_dict = [{}, {}, {}]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 1. only pre-trained upstream"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "# T5 모든 파라미터 동결\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "eval_time = 0\n",
    "test_average_loss = 0\n",
    "test_acc = 0\n",
    "test_perplexity = 0\n",
    "test_bleu_score = 0\n",
    "\n",
    "checkpoint_path = 'checkpoint_t5_1.pth'\n",
    "\n",
    "evaluated = False\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    eval_time = checkpoint['eval_time']\n",
    "    test_average_loss = checkpoint['test_average_loss']\n",
    "    test_acc = checkpoint['test_acc']\n",
    "    test_perplexity = checkpoint['test_perplexity']\n",
    "    test_bleu_score = checkpoint['test_bleu_score']\n",
    "    evaluated = True\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"평가 시작~\")\n",
    "\n",
    "# 훈련 없이 평가만 진행\n",
    "if not evaluated:\n",
    "    start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_average_loss, test_perplexity, test_acc, test_bleu_score = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "    end_time = time.time()\n",
    "    eval_time = end_time - start_time\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    torch.save({\n",
    "        'eval_time': eval_time,\n",
    "        'test_average_loss': test_average_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'test_perplexity': test_perplexity,\n",
    "        'test_bleu_score': test_bleu_score\n",
    "    }, checkpoint_path)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Time: {eval_time:.2f} seconds |\"\n",
    "f\" Test Loss: {test_average_loss:.2f} |\"\n",
    "f\" Test Acc: {test_acc:.3f} |\"\n",
    "f\" Test Perplexity: {test_perplexity:.2f} |\"\n",
    "f\" Test BLEU Score: {test_bleu_score:.2f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그래프 크기 설정\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 작은 값 그래프 (Loss, Accuracy, BLEU)\n",
    "small_metrics = ['Loss', 'Accuracy', 'BLEU']\n",
    "small_values = to_float_list([test_average_loss, test_acc, test_bleu_score])\n",
    "small_colors = ['red', 'blue', 'orange']\n",
    "\n",
    "rects1 = axes[0].bar(small_metrics, small_values, color=small_colors)\n",
    "labels1 = axes[0].bar_label(rects1, fmt='%.2f', padding=3)\n",
    "\n",
    "for label, color in zip(labels1, small_colors):\n",
    "    label.set_color(color)\n",
    "\n",
    "axes[0].set_title(\"Metrics (Small Values)\")\n",
    "axes[0].set_xlabel(\"Metrics\")\n",
    "axes[0].set_ylabel(\"Value\")\n",
    "axes[0].set_ylim(0, max(small_values) + 5)\n",
    "axes[0].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# 큰 값 그래프 (Perplexity)\n",
    "large_metrics = ['Perplexity']\n",
    "large_values =  to_float_list([test_perplexity])\n",
    "large_colors = ['green']\n",
    "\n",
    "rects2 = axes[1].bar(large_metrics, large_values, color=large_colors)\n",
    "labels2 = axes[1].bar_label(rects2, fmt='%.2f', padding=3)\n",
    "\n",
    "for label, color in zip(labels2, large_colors):\n",
    "    label.set_color(color)\n",
    "\n",
    "axes[1].set_title(\"Metrics (Large Value)\")\n",
    "axes[1].set_xlabel(\"Metrics\")\n",
    "axes[1].set_ylabel(\"Value\")\n",
    "axes[1].set_ylim(0, max(large_values) + 50000)\n",
    "axes[1].grid(True, axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# 그래프 저장 및 표시\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/result1.png', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 2. none-trained + Full Training\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "config = T5Config.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration(config).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 10\n",
    "start_epoch = 0\n",
    "\n",
    "time_list = []\n",
    "train_average_loss_list = []\n",
    "test_average_loss_list = []\n",
    "test_accuracies = []\n",
    "train_perplexity_list = []\n",
    "test_perplexity_list = []\n",
    "test_bleu_scores = []\n",
    "\n",
    "checkpoint_path = 'checkpoint_t5_2.pth'\n",
    "\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']  # 이어서 시작할 에포크\n",
    "    time_list = checkpoint['time_list']\n",
    "    train_average_loss_list = checkpoint['train_average_loss_list']\n",
    "    test_average_loss_list = checkpoint['test_average_loss_list']\n",
    "    test_accuracies = checkpoint['test_accuracies']\n",
    "    train_perplexity_list = checkpoint['train_perplexity_list']\n",
    "    test_perplexity_list = checkpoint['test_perplexity_list']\n",
    "    test_bleu_scores = checkpoint['test_bleu_scores']\n",
    "    for epoch in range(0, start_epoch):\n",
    "        print(f\"Epoch {epoch+1:3d} |\"\n",
    "        f\" Time: {time_list[epoch]:.2f} seconds |\"\n",
    "        f\" Train Loss: {train_average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Test Loss: {test_average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Test Acc: {test_accuracies[epoch]:.3f} |\"\n",
    "        f\" Train Perplexity: {train_perplexity_list[epoch]:.2f} |\"\n",
    "        f\" Test Perplexity: {test_perplexity_list[epoch]:.2f} |\"\n",
    "        f\" Test BLEU Score: {test_bleu_scores[epoch]:.2f}\")\n",
    "\n",
    "    if start_epoch < n_epochs -1:\n",
    "        print(f\"이어서 시작~ {start_epoch + 1}.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"새롭게 시작~\")\n",
    "\n",
    "\n",
    "# 훈련 루프 수정\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader):  # tqdm으로 진행 상황 시각화\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # T5 모델에서 직접 loss 계산\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # T5는 자체적으로 loss 반환\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "\n",
    "    # 한 에포크 끝난 뒤, 평균 학습 손실 & PPL\n",
    "    train_average_loss = total_train_loss / len(train_loader)\n",
    "    train_perplexity = torch.exp(torch.tensor(train_average_loss))\n",
    "    train_average_loss_list.append(train_average_loss)\n",
    "    train_perplexity_list.append(train_perplexity)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # 정확도, BLEU 계산\n",
    "        test_average_loss, test_perplexity, test_acc, test_bleu_score = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "        test_average_loss_list.append(test_average_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_bleu_scores.append(test_bleu_score)\n",
    "        test_perplexity_list.append(test_perplexity)\n",
    "\n",
    "        # 에포크 실행 시간 계산\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        time_list.append(epoch_time)\n",
    "\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'time_list': time_list,\n",
    "        'train_average_loss_list': train_average_loss_list,\n",
    "        'test_average_loss_list': test_average_loss_list,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'train_perplexity_list': train_perplexity_list,\n",
    "        'test_perplexity_list': test_perplexity_list,\n",
    "        'test_bleu_scores': test_bleu_scores\n",
    "\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Epoch {epoch+1:3d} |\"\n",
    "    f\" Time: {epoch_time:.2f} seconds |\"\n",
    "    f\" Train Loss: {train_average_loss:.2f} |\"\n",
    "    f\" Test Loss: {test_average_loss:.2f} |\"\n",
    "    f\" Test Acc: {test_acc:.3f} |\"\n",
    "    f\" Train Perplexity: {train_perplexity:.2f} |\"\n",
    "    f\" Test Perplexity: {test_perplexity:.2f} |\"\n",
    "    f\" Test BLEU Score: {test_bleu_score:.2f}\")\n",
    "\n",
    "\n",
    "# 서브플롯 생성 (2행 2열)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2x2 서브플롯\n",
    "\n",
    "# 서브플롯 위치에 그래프 그리기\n",
    "plot_acc(axes[0, 0], \"Loss\", train_average_loss_list, test_average_loss_list)\n",
    "plot_acc(axes[0, 1], \"Perplexity\", train_perplexity_list, test_perplexity_list)\n",
    "plot_acc(axes[1, 0], \"Accuracy\", None, test_accuracies)\n",
    "plot_acc(axes[1, 1], \"BLEU Score\", None, test_bleu_scores)\n",
    "\n",
    "# 간격 조정\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/result2.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "idx = 0\n",
    "metric_dict[idx]['time_list'] = time_list\n",
    "metric_dict[idx]['train_average_loss_list'] = train_average_loss_list\n",
    "metric_dict[idx]['test_average_loss_list'] = test_average_loss_list\n",
    "metric_dict[idx]['test_accuracies'] = test_accuracies\n",
    "metric_dict[idx]['test_perplexity_list'] = test_perplexity_list\n",
    "metric_dict[idx]['test_bleu_scores'] = test_bleu_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 3. pre-trained + Decoder Fine-tuning (+lm_head)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "# 인코더 가중치 고정 (디코더만 학습)\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False  # 인코더 가중치 고정\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 10\n",
    "start_epoch = 0\n",
    "\n",
    "time_list = []\n",
    "train_average_loss_list = []\n",
    "test_average_loss_list = []\n",
    "test_accuracies = []\n",
    "train_perplexity_list = []\n",
    "test_perplexity_list = []\n",
    "test_bleu_scores = []\n",
    "\n",
    "checkpoint_path = 'checkpoint_t5_3.pth'\n",
    "\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']  # 이어서 시작할 에포크\n",
    "    time_list = checkpoint['time_list']\n",
    "    train_average_loss_list = checkpoint['train_average_loss_list']\n",
    "    test_average_loss_list = checkpoint['test_average_loss_list']\n",
    "    test_accuracies = checkpoint['test_accuracies']\n",
    "    train_perplexity_list = checkpoint['train_perplexity_list']\n",
    "    test_perplexity_list = checkpoint['test_perplexity_list']\n",
    "    test_bleu_scores = checkpoint['test_bleu_scores']\n",
    "    for epoch in range(0, start_epoch):\n",
    "        print(f\"Epoch {epoch+1:3d} |\"\n",
    "        f\" Time: {time_list[epoch]:.2f} seconds |\"\n",
    "        f\" Train Loss: {train_average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Test Loss: {test_average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Test Acc: {test_accuracies[epoch]:.3f} |\"\n",
    "        f\" Train Perplexity: {train_perplexity_list[epoch]:.2f} |\"\n",
    "        f\" Test Perplexity: {test_perplexity_list[epoch]:.2f} |\"\n",
    "        f\" Test BLEU Score: {test_bleu_scores[epoch]:.2f}\")\n",
    "\n",
    "    if start_epoch < n_epochs -1:\n",
    "        print(f\"이어서 시작~ {start_epoch + 1}.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"새롭게 시작~\")\n",
    "\n",
    "\n",
    "# 훈련 루프 수정\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader):  # tqdm으로 진행 상황 시각화\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # T5 모델에서 직접 loss 계산\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # T5는 자체적으로 loss 반환\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "\n",
    "    # 한 에포크 끝난 뒤, 평균 학습 손실 & PPL\n",
    "    train_average_loss = total_train_loss / len(train_loader)\n",
    "    train_perplexity = torch.exp(torch.tensor(train_average_loss))\n",
    "    train_average_loss_list.append(train_average_loss)\n",
    "    train_perplexity_list.append(train_perplexity)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # 정확도, BLEU 계산\n",
    "        test_average_loss, test_perplexity, test_acc, test_bleu_score = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "        test_average_loss_list.append(test_average_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_bleu_scores.append(test_bleu_score)\n",
    "        test_perplexity_list.append(test_perplexity)\n",
    "\n",
    "        # 에포크 실행 시간 계산\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        time_list.append(epoch_time)\n",
    "\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'time_list': time_list,\n",
    "        'train_average_loss_list': train_average_loss_list,\n",
    "        'test_average_loss_list': test_average_loss_list,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'train_perplexity_list': train_perplexity_list,\n",
    "        'test_perplexity_list': test_perplexity_list,\n",
    "        'test_bleu_scores': test_bleu_scores\n",
    "\n",
    "    }, checkpoint_path)\n",
    "\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Epoch {epoch+1:3d} |\"\n",
    "    f\" Time: {epoch_time:.2f} seconds |\"\n",
    "    f\" Train Loss: {train_average_loss:.2f} |\"\n",
    "    f\" Test Loss: {test_average_loss:.2f} |\"\n",
    "    f\" Test Acc: {test_acc:.3f} |\"\n",
    "    f\" Train Perplexity: {train_perplexity:.2f} |\"\n",
    "    f\" Test Perplexity: {test_perplexity:.2f} |\"\n",
    "    f\" Test BLEU Score: {test_bleu_score:.2f}\")\n",
    "\n",
    "\n",
    "# 서브플롯 생성 (2행 2열)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2x2 서브플롯\n",
    "\n",
    "# 서브플롯 위치에 그래프 그리기\n",
    "plot_acc(axes[0, 0], \"Loss\", train_average_loss_list, test_average_loss_list)\n",
    "plot_acc(axes[0, 1], \"Perplexity\", train_perplexity_list, test_perplexity_list)\n",
    "plot_acc(axes[1, 0], \"Accuracy\", None, test_accuracies)\n",
    "plot_acc(axes[1, 1], \"BLEU Score\", None, test_bleu_scores)\n",
    "\n",
    "# 간격 조정\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/result3.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "idx = 1\n",
    "metric_dict[idx]['time_list'] = time_list\n",
    "metric_dict[idx]['train_average_loss_list'] = train_average_loss_list\n",
    "metric_dict[idx]['test_average_loss_list'] = test_average_loss_list\n",
    "metric_dict[idx]['test_accuracies'] = test_accuracies\n",
    "metric_dict[idx]['test_perplexity_list'] = test_perplexity_list\n",
    "metric_dict[idx]['test_bleu_scores'] = test_bleu_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✅ 4. pre-trained + Full Fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 10\n",
    "start_epoch = 0\n",
    "\n",
    "time_list = []\n",
    "train_average_loss_list = []\n",
    "test_average_loss_list = []\n",
    "test_accuracies = []\n",
    "train_perplexity_list = []\n",
    "test_perplexity_list = []\n",
    "test_bleu_scores = []\n",
    "\n",
    "checkpoint_path = 'checkpoint_t5_4.pth'\n",
    "\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']  # 이어서 시작할 에포크\n",
    "    time_list = checkpoint['time_list']\n",
    "    train_average_loss_list = checkpoint['train_average_loss_list']\n",
    "    test_average_loss_list = checkpoint['test_average_loss_list']\n",
    "    test_accuracies = checkpoint['test_accuracies']\n",
    "    train_perplexity_list = checkpoint['train_perplexity_list']\n",
    "    test_perplexity_list = checkpoint['test_perplexity_list']\n",
    "    test_bleu_scores = checkpoint['test_bleu_scores']\n",
    "    for epoch in range(0, start_epoch):\n",
    "        print(f\"Epoch {epoch+1:3d} |\"\n",
    "        f\" Time: {time_list[epoch]:.2f} seconds |\"\n",
    "        f\" Train Loss: {train_average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Test Loss: {test_average_loss_list[epoch]:.2f} |\"\n",
    "        f\" Test Acc: {test_accuracies[epoch]:.3f} |\"\n",
    "        f\" Train Perplexity: {train_perplexity_list[epoch]:.2f} |\"\n",
    "        f\" Test Perplexity: {test_perplexity_list[epoch]:.2f} |\"\n",
    "        f\" Test BLEU Score: {test_bleu_scores[epoch]:.2f}\")\n",
    "\n",
    "    if start_epoch < n_epochs -1:\n",
    "        print(f\"이어서 시작~ {start_epoch + 1}.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"새롭게 시작~\")\n",
    "\n",
    "\n",
    "# 훈련 루프 수정\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    start_time = time.time()  # 에포크 시작 시간 기록\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader):  # tqdm으로 진행 상황 시각화\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # T5 모델에서 직접 loss 계산\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # T5는 자체적으로 loss 반환\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "\n",
    "    # 한 에포크 끝난 뒤, 평균 학습 손실 & PPL\n",
    "    train_average_loss = total_train_loss / len(train_loader)\n",
    "    train_perplexity = torch.exp(torch.tensor(train_average_loss))\n",
    "    train_average_loss_list.append(train_average_loss)\n",
    "    train_perplexity_list.append(train_perplexity)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # 정확도, BLEU 계산\n",
    "        test_average_loss, test_perplexity, test_acc, test_bleu_score = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "        test_average_loss_list.append(test_average_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_bleu_scores.append(test_bleu_score)\n",
    "        test_perplexity_list.append(test_perplexity)\n",
    "\n",
    "        # 에포크 실행 시간 계산\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        time_list.append(epoch_time)\n",
    "\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'time_list': time_list,\n",
    "        'train_average_loss_list': train_average_loss_list,\n",
    "        'test_average_loss_list': test_average_loss_list,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'train_perplexity_list': train_perplexity_list,\n",
    "        'test_perplexity_list': test_perplexity_list,\n",
    "        'test_bleu_scores': test_bleu_scores\n",
    "\n",
    "    }, checkpoint_path)\n",
    "\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Epoch {epoch+1:3d} |\"\n",
    "    f\" Time: {epoch_time:.2f} seconds |\"\n",
    "    f\" Train Loss: {train_average_loss:.2f} |\"\n",
    "    f\" Test Loss: {test_average_loss:.2f} |\"\n",
    "    f\" Test Acc: {test_acc:.3f} |\"\n",
    "    f\" Train Perplexity: {train_perplexity:.2f} |\"\n",
    "    f\" Test Perplexity: {test_perplexity:.2f} |\"\n",
    "    f\" Test BLEU Score: {test_bleu_score:.2f}\")\n",
    "\n",
    "# 서브플롯 생성 (2행 2열)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2x2 서브플롯\n",
    "\n",
    "# 서브플롯 위치에 그래프 그리기\n",
    "plot_acc(axes[0, 0], \"Loss\", train_average_loss_list, test_average_loss_list)\n",
    "plot_acc(axes[0, 1], \"Perplexity\", train_perplexity_list, test_perplexity_list)\n",
    "plot_acc(axes[1, 0], \"Accuracy\", None, test_accuracies)\n",
    "plot_acc(axes[1, 1], \"BLEU Score\", None, test_bleu_scores)\n",
    "\n",
    "# 간격 조정\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/result4.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "## t5-small 모델 사용\n",
    "# 2. 번역할 문장\n",
    "text_to_translate = \"hello\"\n",
    "prompt = f\"{text_to_translate}\"\n",
    "\n",
    "# 3. 토크나이즈\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 4. 모델 추론 (generate)\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=40,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "\n",
    "# 5. 결과 디코딩\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 6. 출력\n",
    "print(\"Input Text:\", text_to_translate)\n",
    "print(\"Translated Text (English → French):\", translated_text)\n",
    "\n",
    "idx = 2\n",
    "metric_dict[idx]['time_list'] = time_list\n",
    "metric_dict[idx]['train_average_loss_list'] = train_average_loss_list\n",
    "metric_dict[idx]['test_average_loss_list'] = test_average_loss_list\n",
    "metric_dict[idx]['test_accuracies'] = test_accuracies\n",
    "metric_dict[idx]['test_perplexity_list'] = test_perplexity_list\n",
    "metric_dict[idx]['test_bleu_scores'] = test_bleu_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [LOG] 전체 결과 비교"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors = ['gray', 'green', 'red']\n",
    "linestyles = ['-', '--', '-.']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "plt.figure(figsize=(11, 2))\n",
    "\n",
    "plt.title(\"Epoch Time\")\n",
    "\n",
    "total_times = []\n",
    "\n",
    "for i, label in enumerate(['none-trained + Full Training',\n",
    "                           'pre-trained + Decoder Fine-tuning',\n",
    "                           'pre-trained + Full Fine-tuning']):\n",
    "    time_list = metric_dict[i]['time_list']\n",
    "    total_seconds = sum(time_list)  # 총 시간(초)\n",
    "    hours = total_seconds // 3600  # 시간 계산\n",
    "    minutes = (total_seconds % 3600) // 60  # 나머지 분 계산\n",
    "    total_times.append((hours, minutes))  # 시간, 분 튜플 저장\n",
    "\n",
    "    # 그래프 그리기\n",
    "    plt.plot(\n",
    "             time_list,\n",
    "             label=f'{label} (Total: {hours}h {minutes}m)',\n",
    "             color=colors[i], linestyle=linestyles[i], marker=markers[i], alpha=0.8)\n",
    "\n",
    "    # 주석으로 총합 표시 (시간, 분 단위)\n",
    "    plt.text(len(time_list), time_list[-1], f'{hours}h {minutes}m',\n",
    "             color=colors[i], fontsize=10, ha='right', va='bottom')\n",
    "\n",
    "plt.xticks(range(1, 11))  # x축 눈금을 1부터 10까지 설정\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('./images/epoch_time_hr_min.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 서브플롯 위치에 그래프 그리기\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2x2 서브플롯\n",
    "\n",
    "# 서브플롯 위치에 그래프 그리기\n",
    "plot_acc3(axes[0, 0], \"Loss\", metric_dict[0]['test_average_loss_list'], metric_dict[1]['test_average_loss_list'], metric_dict[2]['test_average_loss_list'])\n",
    "\n",
    "plot_acc3(axes[0, 1], \"Accuracy\", metric_dict[0]['test_accuracies'], metric_dict[1]['test_accuracies'], metric_dict[2]['test_accuracies'])\n",
    "\n",
    "plot_acc3(axes[1, 0], \"Perplexity\", metric_dict[0]['test_perplexity_list'], metric_dict[1]['test_perplexity_list'], metric_dict[2]['test_perplexity_list'])\n",
    "\n",
    "plot_acc3(axes[1, 1], \"BLEU Score\", metric_dict[0]['test_bleu_scores'], metric_dict[1]['test_bleu_scores'], metric_dict[2]['test_bleu_scores'])\n",
    "\n",
    "#plot_acc(axes[0, 1], \"Perplexity\", train_perplexity_list, test_perplexity_list)\n",
    "#plot_acc(axes[1, 0], \"Accuracy\", None, test_accuracies)\n",
    "#plot_acc(axes[1, 1], \"BLEU Score\", None, test_bleu_scores)\n",
    "\n",
    "# 간격 조정\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/entire_result.png', dpi=300)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
